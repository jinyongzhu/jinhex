{"meta":{"title":"技术博客 - Hexo","subtitle":"Hexo Blog","description":"个人技术博客，记录在日常工作中的技术问题的点点滴滴。","author":"Jin yongzhu","url":"https://jinyongzhu.github.io","root":"/"},"pages":[{"title":"关于","date":"2019-11-19T10:36:14.000Z","updated":"2019-11-21T06:47:10.424Z","comments":true,"path":"about/index.html","permalink":"https://jinyongzhu.github.io/about/index.html","excerpt":"","text":""},{"title":"书单","date":"2019-11-19T09:55:31.108Z","updated":"2019-11-19T09:55:31.108Z","comments":false,"path":"books/index.html","permalink":"https://jinyongzhu.github.io/books/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2019-11-19T09:55:31.109Z","updated":"2019-11-19T09:55:31.109Z","comments":true,"path":"links/index.html","permalink":"https://jinyongzhu.github.io/links/index.html","excerpt":"","text":""},{"title":"分类","date":"2019-11-22T01:50:29.000Z","updated":"2019-11-22T03:30:48.269Z","comments":true,"path":"categories/index.html","permalink":"https://jinyongzhu.github.io/categories/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2019-11-19T09:55:31.109Z","updated":"2019-11-19T09:55:31.109Z","comments":false,"path":"repository/index.html","permalink":"https://jinyongzhu.github.io/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-11-22T01:52:29.000Z","updated":"2019-11-22T03:31:00.702Z","comments":true,"path":"tags/index.html","permalink":"https://jinyongzhu.github.io/tags/index.html","excerpt":"","text":""},{"title":"timeline","date":"2019-11-20T07:35:21.000Z","updated":"2019-11-20T07:35:21.564Z","comments":true,"path":"timeline/index.html","permalink":"https://jinyongzhu.github.io/timeline/index.html","excerpt":"","text":""}],"posts":[{"title":"GET 和 POST 到底有什么区别？","slug":"GET-和-POST-到底有什么区别？","date":"2019-11-22T03:13:39.000Z","updated":"2019-11-22T03:25:15.959Z","comments":true,"path":"2019/1122/GET-和-POST-到底有什么区别？/","link":"","permalink":"https://jinyongzhu.github.io/2019/1122/GET-%E5%92%8C-POST-%E5%88%B0%E5%BA%95%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%9F/","excerpt":"","text":"作者：大宽宽 盈米科技（北京）有限公司 高级架构师 链接：https://www.zhihu.com/question/28586791/answer/767316172 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 这个问题虽然看上去很初级，但实际上却涉及到方方面面，这也就是为啥面试里老爱问这个的原因之一。 HTTP最早被用来做浏览器与服务器之间交互HTML和表单的通讯协议；后来又被被广泛的扩充到接口格式的定义上。所以在讨论GET和POST区别的时候，需要现确定下到底是浏览器使用的GET/POST还是用HTTP作为接口传输协议的场景。 浏览器的GET和POST 这里特指浏览器中非Ajax的HTTP请求，即从HTML和浏览器诞生就一直使用的HTTP协议中的GET/POST。浏览器用GET请求来获取一个html页面/图片/css/js等资源；用POST来提交一个表单，并得到一个结果的网页。 浏览器将GET和POST定义为： GET “读取“一个资源。比如Get到一个html文件。反复读取不应该对访问的数据有副作用。比如”GET一下，用户就下单了，返回订单已受理“，这是不可接受的。没有副作用被称为“幂等“（Idempotent)。 因为GET因为是读取，就可以对GET请求的数据做缓存。这个缓存可以做到浏览器本身上（彻底避免浏览器发请求），也可以做到代理上（如nginx），或者做到server端（用Etag，至少可以减少带宽消耗） POST 在页面里 标签会定义一个表单。点击其中的submit元素会发出一个POST请求让服务器做一件事。这件事往往是有副作用的，不幂等的。 不幂等也就意味着不能随意多次执行。因此也就不能缓存。比如通过POST下一个单，服务器创建了新的订单，然后返回订单成功的界面。这个页面不能被缓存。试想一下，如果POST请求被浏览器缓存了，那么下单请求就可以不向服务器发请求，而直接返回本地缓存的“下单成功界面”，却又没有真的在服务器下单。那是一件多么滑稽的事情。 因为POST可能有副作用，所以浏览器实现为不能把POST请求保存为书签。想想，如果点一下书签就下一个单，是不是很恐怖？。 此外如果尝试重新执行POST请求，浏览器也会弹一个框提示下这个刷新可能会有副作用，询问要不要继续。 在chrome中尝试重新提交表单会弹框。 当然，服务器的开发者完全可以把GET实现为有副作用；把POST实现为没有副作用。只不过这和浏览器的预期不符。把GET实现为有副作用是个很可怕的事情。 我依稀记得很久之前百度贴吧有一个因为使用GET请求可以修改管理员的权限而造成的安全漏洞。反过来，把没有副作用的请求用POST实现，浏览器该弹框还是会弹框，对用户体验好处改善不大。 但是后边可以看到，将HTTP POST作为接口的形式使用时，就没有这种弹框了。于是把一个POST请求实现为幂等就有实际的意义。POST幂等能让很多业务的前后端交互更顺畅，以及避免一些因为前端bug，触控失误等带来的重复提交。将一个有副作用的操作实现为幂等必须得从业务上能定义出怎么就算是“重复”。如提交数据中增加一个dedupKey在一个交易会话中有效，或者利用提交的数据里可以天然当dedupKey的字段。这样万一用户强行重复提交，服务器端可以做一次防护。 GET和POST携带数据的格式也有区别。当浏览器发出一个GET请求时，就意味着要么是用户自己在浏览器的地址栏输入，要不就是点击了html里a标签的href中的url。所以其实并不是GET只能用url，而是浏览器直接发出的GET只能由一个url触发。所以没办法，GET上要在url之外带一些参数就只能依靠url上附带querystring。但是HTTP协议本身并没有这个限制。 浏览器的POST请求都来自表单提交。每次提交，表单的数据被浏览器用编码到HTTP请求的body里。浏览器发出的POST请求的body主要有有两种格式，一种是application/x-www-form-urlencoded用来传输简单的数据，大概就是&quot;key1=value1&amp;key2=value2&quot;这样的格式。另外一种是传文件，会采用multipart/form-data格式。采用后者是因为application/x-www-form-urlencoded的编码方式对于文件这种二进制的数据非常低效。 浏览器在POST一个表单时，url上也可以带参数，只要里的url带querystring就行。只不过表单里面的那些用 等标签经过用户操作产生的数据都在会在body里。 因此我们一般会泛泛的说“GET请求没有body，只有url，请求数据放在url的querystring中；POST请求的数据在body中“。但这种情况仅限于浏览器发请求的场景。 接口中的GET和POST 这里是指通过浏览器的Ajax api，或者iOS/Android的App的http client，java的commons-httpclient/okhttp或者是curl，postman之类的工具发出来的GET和POST请求。此时GET/POST不光能用在前端和后端的交互中，还能用在后端各个子服务的调用中（即当一种RPC协议使用）。尽管RPC有很多协议，比如thrift，grpc，但是http本身已经有大量的现成的支持工具可以使用，并且对人类很友好，容易debug。HTTP协议在微服务中的使用是相当普遍的。 当用HTTP实现接口发送请求时，就没有浏览器中那么多限制了，只要是符合HTTP格式的就可以发。HTTP请求的格式，大概是这样的一个字符串（为了美观，我在\\r\\n后都换行一下）： HTTP/1.1\\r\\n : \\r\\n : \\r\\n … : \\r\\n \\r\\n 其中的“\"可以是GET也可以是POST，或者其他的HTTP Method，如PUT、DELETE、OPTION……。从协议本身看，并没有什么限制说GET一定不能没有body，POST就一定不能把参放到的querystring上。因此其实可以更加自由的去利用格式。比如Elastic Search的_search api就用了带body的GET；也可以自己开发接口让POST一半的参数放在url的querystring里，另外一半放body里；你甚至还可以让所有的参数都放Header里——可以做各种各样的定制，只要请求的客户端和服务器端能够约定好。 当然，太自由也带来了另一种麻烦，开发人员不得不每次讨论确定参数是放url的path里，querystring里，body里，header里这种问题，太低效了。于是就有了一些列接口规范/风格。其中名气最大的当属REST。REST充分运用GET、POST、PUT和DELETE，约定了这4个接口分别获取、创建、替换和删除“资源”，REST最佳实践还推荐在请求体使用json格式。这样仅仅通过看HTTP的method就可以明白接口是什么意思，并且解析格式也得到了统一。 json相对于x-www-form-urlencoded的优势在于1）可以有嵌套结构；以及 2）可以支持更丰富的数据类型。通过一些框架，json可以直接被服务器代码映射为业务实体。用起来十分方便。但是如果是写一个接口支持上传文件，那么还是multipart/form-data格式更合适。 REST中GET和POST不是随便用的。在REST中, 【GET】 + 【资源定位符】被专用于获取资源或者资源列表，比如： GET http://foo.com/books 获取书籍列表 GET http://foo.com/books/:bookId 根据bookId获取一本具体的书 与浏览器的场景类似，REST GET也不应该有副作用，于是可以被反复无脑调用。浏览器（包括浏览器的Ajax请求）对于这种GET也可以实现缓存（如果服务器端提示了明确需要Caching）；但是如果用非浏览器，有没有缓存完全看客户端的实现了。当然，也可以从整个App角度，也可以完全绕开浏览器的缓存机制，实现一套业务定制的缓存框架。 ![](v2-d4e4eb5464ee621a83ad3925177ce775_hd.jpg) okhttp中控制Cache的类 REST 【POST】+ 【资源定位符】则用于“创建一个资源”，比如： POST http://foo.com/books { \"title\": \"大宽宽的碎碎念\", \"author\": \"大宽宽\", ... } 这里你就能留意到浏览器中用来实现表单提交的POST，和REST里实现创建资源的POST语义上的不同。 顺便讲下REST POST和REST PUT的区别。有些api是使用PUT作为创建资源的Method。PUT与POST的区别在于，PUT的实际语义是“replace”replace。REST规范里提到PUT的请求体应该是完整的资源，包括id在内。比如上面的创建一本书的api也可以定义为： PUT http://foo.com/books { \"id\": \"BOOK:affe001bbe0556a\", \"title\": \"大宽宽的碎碎念\", \"author\": \"大宽宽\", ... } 服务器应该先根据请求提供的id进行查找，如果存在一个对应id的元素，就用请求中的数据整体替换已经存在的资源；如果没有，就用“把这个id对应的资源从【空】替换为【请求数据】“。直观看起来就是“创建”了。 与PUT相比，POST更像是一个“factory”，通过一组必要的数据创建出完整的资源。 至于到底用PUT还是POST创建资源，完全要看是不是提前可以知道资源所有的数据（尤其是id），以及是不是完整替换。比如对于AWS S3这样的对象存储服务，当想上传一个新资源时，其id就是“ObjectName”可以提前知道；同时这个api也总是完整的replace整个资源。这时的api用PUT的语义更合适；而对于那些id是服务器端自动生成的场景，POST更合适一些。 有点跑题，就此打住。 AWS S3 创建一个Object的API描述 回到接口这个主题，上面仅仅粗略介绍了REST的情况。但是现实中总是有REST的变体，也可能用非REST的协议（比如JSON-RPC、SOAP等），每种情况中的GET和POST又会有所不同。 关于安全性 我们常听到GET不如POST安全，因为POST用body传输数据，而GET用url传输，更加容易看到。但是从攻击的角度，无论是GET还是POST都不够安全，因为HTTP本身是明文协议。每个HTTP请求和返回的每个byte都会在网络上传播，不管是url，header还是body。这完全不是一个“是否容易在浏览器地址栏上看到“的问题。 为了避免传输中数据被窃取，必须做从客户端到服务器的端端加密。业界的通行做法就是https——即用SSL协议协商出的密钥加密明文的http数据。这个加密的协议和HTTP协议本身相互独立。如果是利用HTTP开发公网的站点/App，要保证安全，https是最最基本的要求。 当然，端端加密并不一定非得用https。比如国内金融领域都会用私有网络，也有GB的加密协议SM系列。但除了军队，金融等特殊机构之外，似乎并没有必要自己发明一套类似于ssl的协议。 回到HTTP本身，的确GET请求的参数更倾向于放在url上，因此有更多机会被泄漏。比如携带私密信息的url会展示在地址栏上，还可以分享给第三方，就非常不安全了。此外，从客户端到服务器端，有大量的中间节点，包括网关，代理等。他们的access log通常会输出完整的url，比如nginx的默认access log就是如此。如果url上携带敏感数据，就会被记录下来。但请注意，就算私密数据在body里，也是可以被记录下来的，因此如果请求要经过不信任的公网，避免泄密的唯一手段就是https。这里说的“避免access log泄漏“仅仅是指避免可信区域中的http代理的默认行为带来的安全隐患。比如你是不太希望让自己公司的运维同学从公司主网关的log里看到用户的密码吧。 另外，上面讲过，如果是用作接口，GET实际上也可以带body，POST也可以在url上携带数据。所以实际上到底怎么传输私密数据，要看具体场景具体分析。当然，绝大多数场景，用POST + body里写私密数据是合理的选择。一个典型的例子就是“登录”： POST http://foo.com/user/login { “username”: “dakuankuan”, “passowrd”: “12345678” } 安全是一个巨大的主题，有由很多细节组成的一个完备体系，比如返回私密数据的mask，XSS，CSRF，跨域安全，前端加密，钓鱼，salt，…… POST和GET在安全这件事上仅仅是个小角色。因此单独讨论POST和GET本身哪个更安全意义并不是太大。只要记得一般情况下，私密数据传输用POST + body就好。 关于编码 常见的说法有，比如GET的参数只能支持ASCII，而POST能支持任意binary，包括中文。但其实从上面可以看到，GET和POST实际上都能用url和body。因此所谓编码确切地说应该是http中url用什么编码，body用什么编码。 先说下url。url只能支持ASCII的说法源自于RFC1738 Thus, only alphanumerics, the special characters &quot;−.+!∗′(),&quot;,andreservedcharactersusedfortheirreservedpurposesmaybeusedunencodedwithinaURL.实际上这里规定的仅仅是一个ASCII的子集[a−zA−Z0−9-_.+!*&#x27;(),&quot;, and reserved characters used for their reserved purposes may be used unencoded within a URL. 实际上这里规定的仅仅是一个ASCII的子集[a-zA-Z0-9−.​+!∗′(),&quot;,andreservedcharactersusedfortheirreservedpurposesmaybeusedunencodedwithinaURL.实际上这里规定的仅仅是一个ASCII的子集[a−zA−Z0−9-_.+!*’(),]。它们是可以“不经编码”在url中使用。比如尽管空格也是ASCII字符，但是不能直接用在url里。 那这个“编码”是什么呢？如果有了特殊符号和中文怎么办呢？这里要介绍一种叫做percent encoding的方法： https://en.wikipedia.org/wiki/Percent-encoding​en.wikipedia.org 这也就是为啥我们偶尔看到url里有一坨%和16位数字组成的序列。 使用Percent Encoding，即使是binary data，也是可以通过编码后放在URL上的。 但要特别注意，这个编码方式只管把字符转换成URL可用字符，但是却不管字符集编码（比如中文到底是用UTF8还是GBK）这块早期一直都相当乱，也没有什么统一规范。比如有时跟网页编码一样，有的是操作系统的编码一样。最要命的是浏览器的地址栏是不受开发者控制的这样，对于同样一个带中文的url，如果有的浏览器一定要用GBK（比如老的IE8），有的一定要用UTF8（比如chrome）。后端就可能认不出来。对此常用的办法是避免让用户输入这种带中文的url。如果有这种形式的请求，都改成用户界面上输入，然后通过Ajax发出的办法。Ajax发出的编码形式开发者是可以100%控制的。 不过目前基本上utf8已经大一统了。现在的开发者除非是被国家规定要求一定要用GB系列编码的场景，基本上不会再遇到这类问题了。 关于url的编码，阮一峰的一篇文章有比较详细的解释： 关于URL编码 - 阮一峰的网络日志​www.ruanyifeng.com 顺便说一句，尽管在浏览器地址栏可以看到中文。但这种url在发送请求过程中，浏览器会把中文用字符编码+Percent Encode翻译为真正的url，再发给服务器。浏览器地址栏里的中文只是想让用户体验好些而已。 再讨论下Body。HTTP Body相对好些，因为有个Content-Type来比较明确的定义。比如： POST xxxxxx HTTP/1.1 … Content-Type: application/x-www-form-urlencoded ; charset=UTF-8 这里Content-Type会同时定义请求body的格式（application/x-www-form-urlencoded）和字符编码（UTF-8）。 所以body和url都可以提交中文数据给后端，但是POST的规范好一些，相对不容易出错，容易让开发者安心。对于GET+url的情况，只要不涉及到在老旧浏览器的地址栏输入url，也不会有什么太大的问题。 回到POST，浏览器直接发出的POST请求就是表单提交，而表单提交只有application/x-www-form-urlencoded针对简单的key-value场景；和multipart/form-data，针对只有文件提交，或者同时有文件和key-value的混合提交表单的场景。 如果是Ajax或者其他HTTP Client发出去的POST请求，其body格式就非常自由了，常用的有json，xml，文本，csv……甚至是你自己发明的格式。只要前后端能约定好即可。 浏览器的POST需要发两个请求吗？ 上文中的&quot;HTTP 格式“清楚的显示了HTTP请求可以被大致分为“请求头”和“请求体”两个部分。使用HTTP时大家会有一个约定，即所有的“控制类”信息应该放在请求头中，具体的数据放在请求体里“。于是服务器端在解析时，总是会先完全解析全部的请求头部。这样，服务器端总是希望能够了解请求的控制信息后，就能决定这个请求怎么进一步处理，是拒绝，还是根据content-type去调用相应的解析器处理数据，或者直接用zero copy转发。 比如在用Java写服务时，请求处理代码总是能从HttpSerlvetRequest里getParameter/Header/url。这些信息都是请求头里的，框架直接就解析了。而对于请求体，只提供了一个inputstream，如果开发人员觉得应该进一步处理，就自己去读取和解析请求体。这就能体现出服务器端对请求头和请求体的不同处理方式。 举个实际的例子，比如写一个上传文件的服务，请求url中包含了文件名称，请求体中是个尺寸为几百兆的压缩二进制流。服务器端接收到请求后，就可以先拿到请求头部，查看用户是不是有权限上传，文件名是不是符合规范等。如果不符合，就不再处理请求体的数据了，直接丢弃。而不用等到整个请求都处理完了再拒绝。 为了进一步优化，客户端可以利用HTTP的Continued协议来这样做：客户端总是先发送所有请求头给服务器，让服务器校验。如果通过了，服务器回复“100 - Continue”，客户端再把剩下的数据发给服务器。如果请求被拒了，服务器就回复个400之类的错误，这个交互就终止了。这样，就可以避免浪费带宽传请求体。但是代价就是会多一次Round Trip。如果刚好请求体的数据也不多，那么一次性全部发给服务器可能反而更好。 基于此，客户端就能做一些优化，比如内部设定一次POST的数据超过1KB就先只发“请求头”，否则就一次性全发。客户端甚至还可以做一些Adaptive的策略，统计发送成功率，如果成功率很高，就总是全部发等等。不同浏览器，不同的客户端（curl，postman）可以有各自的不同的方案。不管怎样做，优化目的总是在提高数据吞吐和降低带宽浪费上做一个折衷。 因此到底是发一次还是发N次，客户端可以很灵活的决定。因为不管怎么发都是符合HTTP协议的，因此我们应该视为这种优化是一种实现细节，而不用扯到GET和POST本身的区别上。更不要当个什么世纪大发现。 到底什么算请求体 看完了上面的内容后，读者也许会对“什么是请求体”感到困惑不已，比如x-www-form-endocded编码的body算不算“请求体”呢？ 从HTTP协议的角度，“请求头”就是Method + URL（含querystring）+ Headers；再后边的都是请求体。 但是从业务角度，如果你把一次请求立即为一个调用的话。比如上面的 POST http://foo.com/books { “title”: “大宽宽的碎碎念”, “author”: “大宽宽”, … } 用Java写大概等价于 createBook(“大宽宽的碎碎念”, “大宽宽”); 那么这一行函数名和两个参数都可以看作是一个请求，不区分头和体。即便用HTTP协议实现，title和author编码到了HTTP请求体中。Java的HttpServletRequest支持用getParameter方法获取x-www-url-form-encoded中的数据，表达的意思就是“请求“的”参数“。 对于HTTP，需要区分【头】和【体】，Http Request和Http Response都这么区分。Http这么干主要用作： 对于HTTP代理 支持转发规则，比如nginx先要解析请求头，拿到URL和Header才能决定怎么做（转发proxy_pass，重定向redirect，rewrite后重新判断……） 需要用请求头的信息记录log。尽管请求体里的数据也可以记录，但一般只记录请求头的部分数据。 如果代理规则不涉及到请求体，那么请求体就可以不用从内核态的page cache复制一份到用户态了，可以直接zero copy转发。这对于上传文件的场景极为有效。 …… 对于HTTP服务器 可以通过请求头进行ACL控制，比如看看Athorization头里的数据是否能让认证通过 可以做一些拦截，比如看到Content-Length里的数太大，或者Content-Type自己不支持，或者Accept要求的格式自己无法处理，就直接返回失败了。 如果body的数据很大，利用Stream API，可以方便支持一块一块的处理数据，而不是一次性全部读取出来再操作，以至于占用大量内存。 …… 但从高一级的业务角度，我们在意的其实是【请求】和【返回】。当我们在说“请求头”这三个字时，也许实际的意思是【请求】。而用HTTP实现【请求】时，可能仅仅用到【HTTP的请求头】（比如大部分GET请求），也可能是【HTTP请求头】+【HTTP请求体】（比如用POST实现一次下单）。 总之，这里有两层，不要混哦。 关于URL的长度 因为上面提到了不论是GET和POST都可以使用URL传递数据，所以我们常说的“GET数据有长度限制“其实是指”URL的长度限制“。 HTTP协议本身对URL长度并没有做任何规定。实际的限制是由客户端/浏览器以及服务器端决定的。 先说浏览器。不同浏览器不太一样。比如我们常说的2048个字符的限制，其实是IE8的限制。并且原始文档的说的其实是“URL的最大长度是2083个字符，path的部分最长是2048个字符“。见https://support.microsoft.com/en-us/help/208427/maximum-url-length-is-2-083-characters-in-internet-explorer。IE8之后的IE URL限制我没有查到明确的文档，但有些资料称IE 11的地址栏只能输入法2047个字符，但是允许用户点击html里的超长URL。我没实验，哪位有兴趣可以试试。 Chrome的URL限制是2MB，见https://chromium.googlesource.com/chromium/src/+/master/docs/security/url_display_guidelines/url_display_guidelines.md Safari，Firefox等浏览器也有自己的限制，这里就不挨个列出了。 其他的客户单，比如Java的，js的http client大多数也并没有限制URL最大有多长。 除了浏览器，服务器这边也有限制，比如apache的LimieRequestLine指令。 apache实际上限制的是HTTP请求第一行“Request Line“的长度，即 那一行。 再比如niginx用large_client_header_buffers 指令来分配请求头中的很长数据的buffer。这个buffer可以用来处理url，header value等。 Tomcat的限制是web.xml里maxHttpHeaderSize来设置的，控制的是整个“请求头”的总长度。 为啥要限制呢？如果写过解析一段字符串的代码就能明白，解析的时候要分配内存。对于一个字节流的解析，必须分配buffer来保存所有要存储的数据。而URL这种东西必须当作一个整体看待，无法一块一块处理，于是就处理一个请求时必须分配一整块足够大的内存。如果URL太长，而并发又很高，就容易挤爆服务器的内存；同时，超长URL的好处并不多，我也只有处理老系统的URL时因为不敢碰原来的逻辑，又得追加更多数据，才会使用超长URL。 对于开发者来说，使用超长的URL完全是给自己埋坑，需要同时要考虑前后端，以及中间代理每一个环节的配置。此外，超长URL会影响搜索引擎的爬虫，有些爬虫甚至无法处理超过2000个字节的URL。这也就意味着这些URL无法被搜到，坑爹啊。 其实并没有太大必要弄清楚精确的URL最大长度限制。我个人的经验是，只要某个要开发的资源/api的URL长度有可能达到2000个bytes以上，就必须使用body来传输数据，除非有特殊情况。至于到底是GET + body还是POST + body可以看情况决定。 留意，1个汉字字符经过UTF8编码 + percent encoding后会变成9个字节，别算错哦。 总结 上面讲了一大堆，是希望读者不要死记硬背GET和POST的区别，而是能从更广的层面去看待和思考这个问题。 最后，协议都是人定的。只要客户端和服务器能彼此认同，就能工作。在常规的情况下，用符合规范的方式去实现系统可以减少很多工作量——大家都约定好了，就不要折腾了。但是，总会有一些情况用常规规范不合适，不满足需求。这时思路也不能被规范限制死，更不要死抠RFC。这些规范也许不能处理你遇到的特殊问题。比如： Elastic Search的_search接口使用GET，却用body来表达查询，因为查询很复杂，用querystring很麻烦，必须用json格式才舒服，在请求体用json编码更加容易，不用折腾percent encoding。 用POST写一个接口下单时可能也要考虑幂等，因为前端可能实现“下单按键”有bug，造成用户一次点击发出N个请求。你不能说因为POST by design应该是不幂等就不管了。 协议是死的，人是活的。遇到实际的问题时灵活的运用手上的工具满足需求就好。","categories":[],"tags":[]},{"title":"nginx+php-fpm出现502 bad gateway错误解决方法","slug":"nginx-php-fpm出现502-bad-gateway错误解决方法","date":"2019-11-22T03:08:45.000Z","updated":"2019-11-22T03:09:41.940Z","comments":true,"path":"2019/1122/nginx-php-fpm出现502-bad-gateway错误解决方法/","link":"","permalink":"https://jinyongzhu.github.io/2019/1122/nginx-php-fpm%E5%87%BA%E7%8E%B0502-bad-gateway%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/","excerpt":"","text":"502错误是所有用nginx跑php的运维人员不愿意看见的 nginx出现502有很多原因，但大部分原因可以归结为资源数量不够用,也就是说后端php-fpm处理有问题，nginx将正确的客户端请求发给了后端的php-fpm进程，但是因为php-fpm进程的问题导致不能正确解析php代码，最终返回给了客户端502错误。 服务器出现502的原因是连接超时 我们向服务器发送请求 由于服务器当前链接太多，导致服务器方面无法给于正常的响应,产生此类报错 因此如果你服务器并发量非常大，那只能先增加机器，然后按以下方式优化会取得更好效果;但如果你并发不大却出现502，一般都可以归结为配置问题，脚本超时问题。 1.php-fpm进程数不够用 使用 netstat -napo |grep “php-fpm” | wc -l 查看一下当前fastcgi进程个数，如果个数接近conf里配置的上限，就需要调高进程数。 但也不能无休止调高，可以根据服务器内存情况，可以把php-fpm子进程数调到100或以上，在4G内存的服务器上200就可以。 2. 调高linux内核打开文件数量 可以使用这些命令(必须是root帐号) echo ‘ulimit -HSn 65536′ &gt;&gt; /etc/profile echo ‘ulimit -HSn 65536′ &gt;&gt; /etc/rc.local source /etc/profile 3.脚本执行时间超时 如果脚本因为某种原因长时间等待不返回 ，导致新来的请求不能得到处理，可以适当调小如下配置。 nginx.conf里面主要是如下 fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; php-fpm.conf里如要是如下： request_terminate_timeout = 10s 4.缓存设置比较小 修改或增加配置到nginx.conf proxy_buffer_size 64k; proxy_buffers 512k; proxy_busy_buffers_size 128k; 5. recv() failed (104: Connection reset by peer) while reading response header from upstream 可能的原因机房网络丢包或者机房有硬件防火墙禁止访问该域名 但最重要的是程序里要设置好超时，不要使用php-fpm的request_terminate_timeout， 最好设成request_terminate_timeout=0; 因为这个参数会直接杀掉php进程，然后重启php进程，这样前端nginx就会返回104: Connection reset by peer。这个过程是很慢，总体感觉就是网站很卡。 May 01 10:50:58.044162 [WARNING] [pool www] child 4074, script ‘/usr/local/nginx/html/quancha/sameip/detail.php’ execution timed out (15.129933 sec), terminating May 01 10:50:58.045725 [WARNING] [pool www] child 4074 exited on signal 15 SIGTERM after 90.227060 seconds from start May 01 10:50:58.046818 [NOTICE] [pool www] child 4082 started 说一千道一万最重要的就是程序里控制好超时，gethostbyname、curl、file_get_contents等函数的都要设置超时时间。 另一个就是多说，这个东西是增加了网站的交互性，但是使用的多了反应就慢了，如果你网站超时且使用了多说是，可以关闭它。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jinyongzhu.github.io/categories/Linux/"},{"name":"Nginx","slug":"Linux/Nginx","permalink":"https://jinyongzhu.github.io/categories/Linux/Nginx/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://jinyongzhu.github.io/tags/Linux/"},{"name":"Nginx","slug":"Nginx","permalink":"https://jinyongzhu.github.io/tags/Nginx/"}]},{"title":"CentOS的文件权限与目录配置","slug":"CentOS的文件权限与目录配置","date":"2019-11-22T03:04:53.000Z","updated":"2019-11-22T03:05:49.469Z","comments":true,"path":"2019/1122/CentOS的文件权限与目录配置/","link":"","permalink":"https://jinyongzhu.github.io/2019/1122/CentOS%E7%9A%84%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90%E4%B8%8E%E7%9B%AE%E5%BD%95%E9%85%8D%E7%BD%AE/","excerpt":"","text":"Linux一般将文件可存取访问的身份分为3个类别，分别是owner、group、others，且3种身份各有read、write、execute等权限。 2. 每个账号都可以有多个用户组支持。 3. /etc/passwd，默认情况下所有系统上的账号与一般身份用户还有root的相关信息，都记录在这个文件夹中。 /etc/shadow，个人密码记录在这个文件夹中。 /etc/group，所有的组名记录在这个文件夹中。 Linux文件权限概念 Linux文件属性 1. ls−al，ls是“list”的意思，重点在显示文件的文件名与相关属性。参数“−al”则表示列出所有的文件详细权限与属性（包含隐藏文件，就是文件名第一个字符为“.”的文件）2.显示的每一行会有如下格式，例如：−rw−r–r–1rootroot42304Sep418:26install.log，分别表示：权限连接所有者用户组文件容量修改日期文件名3.权限−rw−r–r–中拥有10个字符，分别代表如下：第1个字符代表文件的类型。“d”代表目录，“−”代表文件，“l”代表链接文件（linkfile），“b”表示设备文件里面的可供存储的接口设备，“c”表示设备文件里面的串行端口设备，例如鼠标键盘（一次性读取设备）。接下来的字符，以3个为一组，且均为“rwx”的3个参数组合，“r”表示可读，“w”表示可写，“x”表示可执行（execute），这三个权限的位置不会改变，如果没有权限，则以减号“−”代替。第一组为文件所有者的权限，第二组为同用户组的权限，第三组为其他非本用户组的权限。4.2.中的“链接”表示有多少文件链接到此节点（i−node）。每隔文件都会将它的权限与属性记录到文件系统的i−node中，每个文件都会链接到一个i−node。这个属性记录的就是有多少个不同的文件名连接到相同的一个i−node号码。5.2.中的“修改日期”表示这个文件的创建日期或最近修改的日期。如果年份太久则只会显示年份。想要显示完整的时间格式，则可使用如下指令（ls的参数）：ls -al，ls是“list”的意思，重点在显示文件的文件名与相关属性。参数“-al”则表示列出所有的文件详细权限与属性（包含隐藏文件，就是文件名第一个字符为“.”的文件） 2. 显示的每一行会有如下格式，例如： -rw-r–r– 1 root root 42304 Sep 4 18:26 install.log，分别表示： 权限 连接 所有者 用户组 文件容量 修改日期 文件名 3. 权限-rw-r–r–中拥有10个字符，分别代表如下： 第1个字符代表文件的类型。“d”代表目录，“-”代表文件，“l”代表链接文件（linkfile），“b”表示设备文件里面的可供存储的接口设备，“c”表示设备文件里面的串行端口设备，例如鼠标键盘（一次性读取设备）。 接 下来的字符，以3个为一组，且均为“rwx”的3个参数组合，“r”表示可读，“w”表示可写，“x”表示可执行（execute），这三个权限的位置不 会改变，如果没有权限，则以减号“-”代替。第一组为文件所有者的权限，第二组为同用户组的权限，第三组为其他非本用户组的权限。 4. 2.中的“链接”表示有多少文件链接到此节点（i-node）。每隔文件都会将它的权限与属性记录到文件系统的i-node中，每个文件都会链接到一个i-node。这个属性记录的就是有多少个不同的文件名连接到相同的一个i-node号码。 5. 2.中的“修改日期”表示这个文件的创建日期或最近修改的日期。如果年份太久则只会显示年份。想要显示完整的时间格式，则可使用如下指令（ls的参数）： ls−al，ls是“list”的意思，重点在显示文件的文件名与相关属性。参数“−al”则表示列出所有的文件详细权限与属性（包含隐藏文件，就是文件名第一个字符为“.”的文件）2.显示的每一行会有如下格式，例如：−rw−r–r–1rootroot42304Sep418:26install.log，分别表示：权限连接所有者用户组文件容量修改日期文件名3.权限−rw−r–r–中拥有10个字符，分别代表如下：第1个字符代表文件的类型。“d”代表目录，“−”代表文件，“l”代表链接文件（linkfile），“b”表示设备文件里面的可供存储的接口设备，“c”表示设备文件里面的串行端口设备，例如鼠标键盘（一次性读取设备）。接下来的字符，以3个为一组，且均为“rwx”的3个参数组合，“r”表示可读，“w”表示可写，“x”表示可执行（execute），这三个权限的位置不会改变，如果没有权限，则以减号“−”代替。第一组为文件所有者的权限，第二组为同用户组的权限，第三组为其他非本用户组的权限。4.2.中的“链接”表示有多少文件链接到此节点（i−node）。每隔文件都会将它的权限与属性记录到文件系统的i−node中，每个文件都会链接到一个i−node。这个属性记录的就是有多少个不同的文件名连接到相同的一个i−node号码。5.2.中的“修改日期”表示这个文件的创建日期或最近修改的日期。如果年份太久则只会显示年份。想要显示完整的时间格式，则可使用如下指令（ls的参数）：ls -l –full-time 6. 修改系统默认语言为英文，可以管理员身份修改该系统配置文件/etc/sysconfig/i18n，利用nano编译器进行修改，LANG=en_US。 #nano /etc/sysconfig/i18n，将LANG后面改为“en_US……” 7. 对于目录的权限如果是“-drwxr-xr–”，则others仍然不能进入本目录，需要有x权限才能进入。 如何改变文件属性与权限 1. chgrp，改变文件所属用户组； chown，改变文件所有者； chmod，改变文件的权限。 2. chgrp就是change group的简称，使用该指令时，要被改变的组名必须在/etc/group文件内存在才行。 #chgrp [-R] group filename(or dirname)，其中R表示进行递归（recursive）的持续更改，也即连同子目录下的所有文件、目录。所以当修改一个目录中所有文件的用户组（所有者与权限也一样）时，要加上-R。 例如将文件install.log改到users用户组 chgrp users install.log 3. chown就是change owner的简称。 #chown [-R] user filename(or dirname)，改变file的文件所有者为user。 #chown [-R] .group filename(or dirname)，改变file的用户组为group（注意加点）。 #chown [-R] user.group filename(or dirname)，改变file的文件所有者为user，用户组为group。为避免“.”引起的系统误判，通常用一下命令表示该句： #chown [-R] user:group filename(or dirname)。 4. 复制文件给其他人，复制命令： cp [-option] [source file or dir] [target file or dir] 复制行为（cp）会复制执行者的属性与权限，所以即使复制到他人用户组仍然无法使用，所以这时必须修改该权限。 5. chmod就是change mode bits的简称。 数字类型改变文件权限： #chmod [-R] xyz fileordir，其中x代表owner权限，y代表group权限，z代表others权限。 r=4，w=2，x=1，上面三种身份的权限是r+w+x的和，如果没有相应的权限，则值为0。 例如：install.log文件，owner=rwx=4+2+1=7，group=rwx=4+2+1=7，others=—=0+0+0=0，所以这个文件的将改变权限值为770： #chmod 770 install.log。 6. 符号类型改变文件权限 我们可以用u，g，o三个参数来代表user，group，others 3种身份的权限。 a代表all，也即全部的身份。 读写的权限就可以写成r，w，x。 +，-，=分别代表加入，出去，设置一个权限。 加入要设置一个文件的权限成“-rwxr-xr-x，指令为： #chmod u=rwx,go=rx filename，注意加上那个逗号。 要给一个文件的全部身份加上x权限，则指令为： #chmod a+x filename。 目录与文件的权限意义 1. 权限对文件的作用： 文件是实际含有数据的地方，包括一般文本文件、数据库内容文件、二进制可执行文件（binary program）等。 r（read）：可读取此文件实际内容。 w（write）：可以编辑、新增或者是修改该文件的内容（不能删除该文件）。 x（execute）：可以被系统执行。 2. 权限对目录的作用： r（read contemts in directory）可读取目录结构列表，可利用ls命令将该目录结构列表中的文件名显示出来（仅能显示文件名）。 w（modify contents of directory）可更改目录结构列表，如新建、删除、重命名、转移文件或目录。 x（access directory）可进入该目录成为工作目录（就是目前所在的目录），当登陆Linux时所在的~（即主文件夹）就是当前工作目录。 3. 工作目录对于命令的执行非常重要，如果对一个目录不具有x权限，则无法使该目录称为工作目录，也就无法执行该目录下的任何命令。 要开放目录给任何人浏览时，应该至少给予r和x的权限，但w的权限不可以随便给。 4. 如果具有对一个目录的w权限，那么该目录下的任何文件都可以删除，不管要删除的文件的所有者和用户组是什么。 5. #cd /tmp，切换工作目录到/tmp #mkdir testing，在以上目录下建立testing文件夹 #chmod 744 testing，更改以上创建的testing文件夹的权限为drwxr–r– #touch testing/testing1，在testing文件夹中新建空的文件testing1 #ls -ald testing testing/testing1，列出上面创建的文件夹和文件的信息（ls的-a参数表示all，即显示所有文件，包括隐藏文件；-l参数表示 long，显示长信息格式，即显示用户组，所有者，修改日期等等；-d参数表示将目录象文件一样显示，而不是显示其下的文件）。 可以看到testing目录，和testing1文件的所有者和用户组都是root。 下面来用自己的账号进行一些操作。 6. #su – SF_Chipan，切换身份成为SF_Chipan cd/tmp，进入/tmp成为工作目录cd /tmp，进入/tmp成为工作目录 cd/tmp，进入/tmp成为工作目录ls -l testing，列出testing目录中的内容，因为刚才在5.中设置了testing目录对others的权限为r，所以这里就会显示文件名为testing1的文件，其他都是问号。 cdtesing，这时候因为SFChipan身份对testing目录的权限只是r，没有x，所以不能使testing成为工作目录。7.使用“su–”登陆的账户，当使用cd tesing，这时候因为SF_Chipan身份对testing目录的权限只是r，没有x，所以不能使testing成为工作目录。 7. 使用“su – ”登陆的账户，当使用cdtesing，这时候因为SFC​hipan身份对testing目录的权限只是r，没有x，所以不能使testing成为工作目录。7.使用“su–”登陆的账户，当使用exit退出时，是退回使用“su – ”之前的账户。如果使用“su – ”登陆root账户，需要提供密码，而root账户登陆到一般账户，则不需要提供密码。 8. 可以是用rm命令删除testing1文件： #cd /tmp/testing #rm testing1 Linux文件种类与扩展名 1. 文件种类： 普通文件（regular file）：[-] 纯文本文件（ASCⅡ）：Linux中最多的一种文件类型。可以执行下列命令来查看一个隐藏文件： $cat ~/.bashrc，cat是将一个文件内容读出来的命令。 二进制文件（binary）：Linux中的可执行文件就是这种格式的（scripts、文字批处理文件不算），.bashrc就是这种格式。 目录（directory）：[d] 链接文件（link）：[l]，有点类似windows下的快捷方式。 设 备与设备文件（device）通常分为一下两种：块（block）设备文件，[ d]，就是一些存储数据以提供系统随机访问的接口设备，如硬盘等。可以随机地在硬盘的不同块读写，这种设备就是成组设备；字符（character）设备 文件，就是一些串行端口的接口设备，如键盘鼠标，特点是“一次性读取”，不能够阶段输出，比如鼠标不可能跳到另一个地方，只能划过去。 套接字（sockets）：[s]，通常被用在网络上的数据连接。我们可以启动一个程序来监听客户端的请求，而客户端就可以通过这个socket来进行书序的通信。通常在/var/run这个目录中就能看到。 管道（FIFO，pipe）：[p]，主要目的在解决多一个程序同时访问一个文件所造成的错误问题。FIFO是first-in-first-out的缩写。 上面的套接字和管道都与进程比较有关，可以通过man fifo及man socket来查阅。 2. 一个Linux文件能不能被执行，与它的第一列的10个属性有关，与文件名一点关系都没有。 3. x权限代表这个文件具有可执行的能力，但能不能执行成功，要看该文件的内容。 4. 我们希望可以通过扩展名了解该文件时什么东西，所以还是会使用适当的扩展名来表示该文件是什么种类的。 5. *.sh：脚本或批处理文件（scripts），因为批处理文件用shell写成的，所以扩展名就是.sh。 Z、.tar、tar.gz、zip、.tgz：经过打包的压缩文件。由于不同的压缩软件而取其相关的扩展名。 .html、.php：网页相关文件，分别代表HTML语法与PHP语法的网页文件。.html的文件可以使用网页浏览器来直接启动；.php的文件，可以通过客户端的浏览器来服务端浏览，以得到运算后的网页结果。 6. 从网络上传送到你的Linux系统中，文件的属性与权限可能是会改变的。 7. 在Linux下，使用默认的Ext2/Ext3文件系统时，针对文件的文件名长度限制为：单一文件或目录的最大容许文件名为255个字符；包含完整路径名称及目录（/）的完整文件名为4096个字符。 8. 由于Linux在文字界面下的一些命令操作的关系，在设置文件名时，最好能避免一些特殊的字符，如：?&gt;&lt;;&amp;![]|’”`(){}。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jinyongzhu.github.io/categories/Linux/"},{"name":"CentOS","slug":"Linux/CentOS","permalink":"https://jinyongzhu.github.io/categories/Linux/CentOS/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://jinyongzhu.github.io/tags/Linux/"},{"name":"CentOS","slug":"CentOS","permalink":"https://jinyongzhu.github.io/tags/CentOS/"}]},{"title":"SHELL中各种括号的作用()、(())、[]、[[]]、{}","slug":"SHELL中各种括号的作用-、-、-、-、","date":"2019-11-22T03:03:44.000Z","updated":"2019-11-22T03:04:26.166Z","comments":true,"path":"2019/1122/SHELL中各种括号的作用-、-、-、-、/","link":"","permalink":"https://jinyongzhu.github.io/2019/1122/SHELL%E4%B8%AD%E5%90%84%E7%A7%8D%E6%8B%AC%E5%8F%B7%E7%9A%84%E4%BD%9C%E7%94%A8-%E3%80%81-%E3%80%81-%E3%80%81-%E3%80%81/","excerpt":"","text":"一、小括号，圆括号() 1、单小括号 () ①命令组。括号中的命令将会新开一个子shell顺序执行，所以括号中的变量不能够被脚本余下的部分使用。括号中多个命令之间用分号隔开，最后一个命令可以没有分号，各命令和括号之间不必有空格。 ②命令替换。等同于cmd，shell扫描一遍命令行，发现了(cmd)结构，便将(cmd)结构，便将(cmd)结构，便将(cmd)中的cmd执行一次，得到其标准输出，再将此输出放到原来命令。有些shell不支持，如tcsh。 ③用于初始化数组。如：array=(a b c d) 2、双小括号 (( )) ①整数扩展。这种扩展计算是整数型的计算，不支持浮点型。((exp))结构扩展并计算一个算术表达式的值，如果表达式的结果为0，那么返回的退出状态码为1，或者 是&quot;假&quot;，而一个非零值的表达式所返回的退出状态码将为0，或者是&quot;true&quot;。若是逻辑判断，表达式exp为真则为1,假则为0。 ②只要括号中的运算符、表达式符合C语言运算规则，都可用在$((exp))中，甚至是三目运算符。作不同进位(如二进制、八进制、十六进制)运算时，输出结果全都自动转化成了十进制。如：echo $((16#5f)) 结果为95 (16进位转十进制) ③单纯用 (( )) 也可重定义变量值，比如 a=5; ((a++)) 可将 a重定义为6 ④常用于算术运算比较，双括号中的变量可以不使用a 重定义为6 ④常用于算术运算比较，双括号中的变量可以不使用a重定义为6 ④常用于算术运算比较，双括号中的变量可以不使用符号前缀。括号内支持多个表达式用逗号分开。 只要括号中的表达式符合C语言运算规则,比如可以直接使用for((i=0;i&lt;5;i++)), 如果不使用双括号, 则为for i in seq 0 4或者for i in {0…4}。再如可以直接使用if (($i&lt;5)), 如果不使用双括号, 则为if [ $i -lt 5 ]。 二、中括号，方括号[] 1、单中括号 [] ①bash 的内部命令，[和test是等同的。如果我们不用绝对路径指明，通常我们用的都是bash自带的命令。if/test结构中的左中括号是调用test的命令标识，右中括号是关闭条件判断的。这个命令把它的参数作为比较表达式或者作为文件测试，并且根据比较的结果来返回一个退出状态码。if/test结构中并不是必须右中括号，但是新版的Bash中要求必须这样。 ②Test和[]中可用的比较运算符只有==和!=，两者都是用于字符串比较的，不可用于整数比较，整数比较只能使用-eq，-gt这种形式。无论是字符串比较还是整数比较都不支持大于号小于号。如果实在想用，对于字符串比较可以使用转义形式，如果比较&quot;ab&quot;和&quot;bc&quot;：[ ab &lt; bc ]，结果为真，也就是返回状态为0。[ ]中的逻辑与和逻辑或使用-a 和-o 表示。 ③字符范围。用作正则表达式的一部分，描述一个匹配的字符范围。作为test用途的中括号内不能使用正则。 ④在一个array 结构的上下文中，中括号用来引用数组中每个元素的编号。 2、双中括号[[ ]] ①[[是 bash 程序语言的关键字。并不是一个命令，[[ ]] 结构比[ ]结构更加通用。在[[和]]之间所有的字符都不会发生文件名扩展或者单词分割，但是会发生参数扩展和命令替换。 ②支持字符串的模式匹配，使用=~操作符时甚至支持shell的正则表达式。字符串比较时可以把右边的作为一个模式，而不仅仅是一个字符串，比如[[ hello == hell? ]]，结果为真。[[ ]] 中匹配字符串或通配符，不需要引号。 ③使用[[ … ]]条件判断结构，而不是[ … ]，能够防止脚本中的许多逻辑错误。比如，&amp;&amp;、||、&lt;和&gt; 操作符能够正常存在于[[ ]]条件判断结构中，但是如果出现在[ ]结构中的话，会报错。比如可以直接使用if [[ $a != 1 &amp;&amp; $a != 2 ]], 如果不适用双括号, 则为if [ $a -ne 1] &amp;&amp; [ $a != 2 ]或者if [ $a -ne 1 -a $a != 2 ]。 ④bash把双中括号中的表达式看作一个单独的元素，并返回一个退出状态码。 例子： if ($i&lt;5) if [ $i -lt 5 ] if [ $a -ne 1 -a $a != 2 ] if [ $a -ne 1] &amp;&amp; [ $a != 2 ] if [[ $a != 1 &amp;&amp; $a != 2 ]] for i in $(seq 0 4);do echo $i;done for i in seq 0 4;do echo $i;done for ((i=0;i&lt;5;i++));do echo $i;done for i in {0…4};do echo $i;done 三、大括号、花括号 {} 1、常规用法 ①大括号拓展。(通配(globbing))将对大括号中的文件名做扩展。在大括号中，不允许有空白，除非这个空白被引用或转义。第一种：对大括号中的以逗号分割的文件列表进行拓展。如 touch {a,b}.txt 结果为a.txt b.txt。第二种：对大括号中以点点（…）分割的顺序文件列表起拓展作用，如：touch {a…d}.txt 结果为a.txt b.txt c.txt d.txt ls {ex1,ex2}.sh ex1.sh ex2.sh ls {ex{1…3},ex4}.sh ex1.sh ex2.sh ex3.sh ex4.sh ls {ex[1-3],ex4}.sh ex1.sh ex2.sh ex3.sh ex4.sh ②代码块，又被称为内部组，这个结构事实上创建了一个匿名函数 。与小括号中的命令不同，大括号内的命令不会新开一个子shell运行，即脚本余下部分仍可使用括号内变量。括号内的命令间用分号隔开，最后一个也必须有分号。{}的第一个命令和左括号之间必须要有一个空格。 2、几种特殊的替换结构 var:−string,{var:-string},var:−string,{var:+string},var:=string,{var:=string},var:=string,{var:?string} ①var:−string和{var:-string}和var:−string和{var:=string}:若变量var为空，则用在命令行中用string来替换var:−string，否则变量var不为空时，则用变量var的值来替换{var:-string}，否则变量var不为空时，则用变量var的值来替换var:−string，否则变量var不为空时，则用变量var的值来替换{var:-string}；对于var:=string的替换规则和{var:=string}的替换规则和var:=string的替换规则和{var:-string}是一样的，所不同之处是var:=string若var为空时，用string替换{var:=string}若var为空时，用string替换var:=string若var为空时，用string替换{var:=string}的同时，把string赋给变量var： ${var:=string}很常用的一种用法是，判断某个变量是否赋值，没有的话则给它赋上一个默认值。 ② var:+string的替换规则和上面的相反，即只有当var不是空的时候才替换成string，若var为空时则不替换或者说是替换成变量var的值，即空值。(因为变量var此时为空，所以这两种说法是等价的)③{var:+string}的替换规则和上面的相反，即只有当var不是空的时候才替换成string，若var为空时则不替换或者说是替换成变量 var的值，即空值。(因为变量var此时为空，所以这两种说法是等价的) ③var:+string的替换规则和上面的相反，即只有当var不是空的时候才替换成string，若var为空时则不替换或者说是替换成变量var的值，即空值。(因为变量var此时为空，所以这两种说法是等价的)③{var:?string}替换规则为：若变量var不为空，则用变量var的值来替换${var:?string}；若变量var为空，则把string输出到标准错误中，并从脚本中退出。我们可利用此特性来检查是否设置了变量的值。 补充扩展：在上面这五种替换结构中string不一定是常值的，可用另外一个变量的值或是一种命令的输出。 3、四种模式匹配替换结构 模式匹配记忆方法： 是去掉左边(在键盘上#在$之左边) % 是去掉右边(在键盘上%在$之右边) #和%中的单一符号是最小匹配，两个相同符号是最大匹配。 {var%pattern},{var%%pattern},{var#pattern},{var##pattern} 第一种模式：${variable%pattern}，这种模式时，shell在variable中查找，看它是否一给的模式pattern结尾，如果是，就从命令行把variable中的内容去掉右边最短的匹配模式 第二种模式： {variable%%pattern}，这种模式时，shell在variable中查找，看它是否一给的模式pattern结尾，如果是，就从命令行把variable中的内容去掉右边最长的匹配模式 第三种模式：{variable#pattern} 这种模式时，shell在variable中查找，看它是否一给的模式pattern开始，如果是，就从命令行把variable中的内容去掉左边最短的匹配模式 第四种模式： ${variable##pattern} 这种模式时，shell在variable中查找，看它是否一给的模式pattern结尾，如果是，就从命令行把variable中的内容去掉右边最长的匹配模式 这四种模式中都不会改变variable的值，其中，只有在pattern中使用了*匹配符号时，%和%%，#和##才有区别。结构中的pattern支持通配符，*表示零个或多个任意字符，?表示仅与一个任意字符匹配，[…]表示匹配中括号里面的字符，[!..]表示不匹配中括号里面的字符。 var=testcase echo $var testcase echo ${var%s*e} testca echo $var testcase echo ${var%%s*e} te echo ${var#?e} stcase echo ${var##?e} stcase echo ${var##*e} echo ${var##*s} e echo ${var##test} case 4、字符串提取和替换 var:num,{var:num},var:num,{var:num1:num2},var/pattern/pattern,{var/pattern/pattern},var/pattern/pattern,{var//pattern/pattern} 第一种模式：var:num，这种模式时，shell在var中提取第num个字符到末尾的所有字符。若num为正数，从左边0处开始；若num为负数，从右边开始提取字串，但必须使用在冒号后面加空格或一个数字或整个num加上括号，如{var:num}，这种模式时，shell在var中提取第num个字符到末尾的所有字符。若num为正数，从左边0处开始；若num为负数，从右边开始提取字串，但必须使用在冒号后面加空格或一个数字或整个num加上括号，如var:num，这种模式时，shell在var中提取第num个字符到末尾的所有字符。若num为正数，从左边0处开始；若num为负数，从右边开始提取字串，但必须使用在冒号后面加空格或一个数字或整个num加上括号，如{var: -2}、var:1−3或{var:1-3}或var:1−3或{var:(-2)}。 第二种模式：var:num1:num2，num1是位置，num2是长度。表示从{var:num1:num2}，num1是位置，num2是长度。表示从var:num1:num2，num1是位置，num2是长度。表示从var字符串的第num1个位置开始提取长度为num1个位置开始提取长度为num1个位置开始提取长度为num2的子串。不能为负数。 第三种模式：var/pattern/pattern表示将var字符串的第一个匹配的pattern替换为另一个pattern。第四种模式：{var/pattern/pattern}表示将var字符串的第一个匹配的pattern替换为另一个pattern。 第四种模式：var/pattern/pattern表示将var字符串的第一个匹配的pattern替换为另一个pattern。第四种模式：{var//pattern/pattern}表示将var字符串中的所有能匹配的pattern替换为另一个pattern。 [root@centos ~]# var=/home/centos [root@centos ~]# echo $var /home/centos [root@centos ~]# echo ${var:5} /centos [root@centos ~]# echo ${var: -6} centos [root@centos ~]# echo ${var:(-6)} centos [root@centos ~]# echo ${var:1:4} home [root@centos ~]# echo ${var/o/h} /hhme/centos [root@centos ~]# echo ${var//o/h} /hhme/cenths 四、符号后的括号（1）后的括号 （1）后的括号（1）{a} 变量a的值, 在不引起歧义的情况下可以省略大括号。 （2）(cmd)命令替换，和‘cmd‘效果相同，结果为shell命令cmd的输，过某些Shell版本不支持(cmd) 命令替换，和`cmd`效果相同，结果为shell命令cmd的输，过某些Shell版本不支持(cmd)命令替换，和‘cmd‘效果相同，结果为shell命令cmd的输，过某些Shell版本不支持()形式的命令替换, 如tcsh。 （3）$((expression)) 和exprexpression效果相同, 计算数学表达式exp的数值, 其中exp只要符合C语言的运算规则即可, 甚至三目运算符和逻辑表达式都可以计算。 五、使用 1、多条命令执行 （1）单小括号，(cmd1;cmd2;cmd3) 新开一个子shell顺序执行命令cmd1,cmd2,cmd3, 各命令之间用分号隔开, 最后一个命令后可以没有分号。 （2）单大括号，{ cmd1;cmd2;cmd3;} 在当前shell顺序执行命令cmd1,cmd2,cmd3, 各命令之间用分号隔开, 最后一个命令后必须有分号, 第一条命令和左括号之间必须用空格隔开。 对{}和()而言, 括号中的重定向符只影响该条命令， 而括号外的重定向符影响到括号中的所有命令。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jinyongzhu.github.io/categories/Linux/"},{"name":"shell","slug":"Linux/shell","permalink":"https://jinyongzhu.github.io/categories/Linux/shell/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://jinyongzhu.github.io/tags/Linux/"},{"name":"shell","slug":"shell","permalink":"https://jinyongzhu.github.io/tags/shell/"}]},{"title":"VIM命令使用大全","slug":"VIM命令使用大全","date":"2019-11-22T03:02:23.000Z","updated":"2019-11-22T03:03:15.909Z","comments":true,"path":"2019/1122/VIM命令使用大全/","link":"","permalink":"https://jinyongzhu.github.io/2019/1122/VIM%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8%E5%A4%A7%E5%85%A8/","excerpt":"","text":"以:和/开头的命令都有历史纪录，可以首先键入:或/然后按上下箭头来选择某个历史命令。 启动vim 在命令行窗口中输入以下命令即可 vim 直接启动vim vim filename 打开vim并创建名为filename的文件 文件命令 打开单个文件 vim file 同时打开多个文件 vim file1 file2 file3 … 在vim窗口中打开一个新文件 :open file 在新窗口中打开文件 :split file 切换到下一个文件 :bn 切换到上一个文件 :bp 查看当前打开的文件列表，当前正在编辑的文件会用[]括起来。 :args 打开远程文件，比如ftp或者share folder :e ftp://192.168.10.76/abc.txt :e \\qadrive\\test\\1.txt vim的模式 正常模式（按Esc或Ctrl+[进入） 左下角显示文件名或为空 插入模式（按i键进入） 左下角显示–INSERT– 可视模式（不知道如何进入） 左下角显示–VISUAL– 导航命令 % 括号匹配 插入命令 i 在当前位置生前插入 I 在当前行首插入 a 在当前位置后插入 A 在当前行尾插入 o 在当前行之后插入一行 O 在当前行之前插入一行 查找命令 /text 查找text，按n健查找下一个，按N健查找前一个。 ?text 查找text，反向查找，按n健查找下一个，按N健查找前一个。 vim中有一些特殊字符在查找时需要转义 .*[]^%/?~ :set ignorecase 忽略大小写的查找 :set noignorecase 不忽略大小写的查找 查找很长的词，如果一个词很长，键入麻烦，可以将光标移动到该词上，按*或#键即可以该单词进行搜索，相当于/搜索。而#命令相当于?搜索。 :set hlsearch 高亮搜索结果，所有结果都高亮显示，而不是只显示一个匹配。 :set nohlsearch 关闭高亮搜索显示 :nohlsearch 关闭当前的高亮显示，如果再次搜索或者按下n或N键，则会再次高亮。 :set incsearch 逐步搜索模式，对当前键入的字符进行搜索而不必等待键入完成。 :set wrapscan 重新搜索，在搜索到文件头或尾时，返回继续搜索，默认开启。 替换命令 ra 将当前字符替换为a，当期字符即光标所在字符。 s/old/new/ 用old替换new，替换当前行的第一个匹配 s/old/new/g 用old替换new，替换当前行的所有匹配 %s/old/new/ 用old替换new，替换所有行的第一个匹配 %s/old/new/g 用old替换new，替换整个文件的所有匹配 :10,20 s/^/ /g 在第10行知第20行每行前面加四个空格，用于缩进。 ddp 交换光标所在行和其下紧邻的一行。 移动命令 h 左移一个字符 l 右移一个字符，这个命令很少用，一般用w代替。 k 上移一个字符 j 下移一个字符 以上四个命令可以配合数字使用，比如20j就是向下移动20行，5h就是向左移动5个字符，在Vim中，很多命令都可以配合数字使用，比如删除10个字符10x，在当前位置后插入3个！，3a！&lt;Esc&gt;，这里的Esc是必须的，否则命令不生效。 w 向前移动一个单词（光标停在单词首部），如果已到行尾，则转至下一行行首。此命令快，可以代替l命令。 b 向后移动一个单词 2b 向后移动2个单词 e，同w，只不过是光标停在单词尾部 ge，同b，光标停在单词尾部。 ^ 移动到本行第一个非空白字符上。 0（数字0）移动到本行第一个字符上， &lt;HOME&gt; 移动到本行第一个字符。同0健。 移动到行尾 3$ 移动到下面3行的行尾 gg 移动到文件头。 = [[ G（shift + g） 移动到文件尾。 = ]] f（find）命令也可以用于移动，fx将找到光标后第一个为x的字符，3fd将找到第三个为d的字符。 F 同f，反向查找。 跳到指定行，冒号+行号，回车，比如跳到240行就是 :240回车。另一个方法是行号+G，比如230G跳到230行。 Ctrl + e 向下滚动一行 Ctrl + y 向上滚动一行 Ctrl + d 向下滚动半屏 Ctrl + u 向上滚动半屏 Ctrl + f 向下滚动一屏 Ctrl + b 向上滚动一屏 撤销和重做 u 撤销（Undo） U 撤销对整行的操作 Ctrl + r 重做（Redo），即撤销的撤销。 删除命令 x 删除当前字符 3x 删除当前光标开始向后三个字符 X 删除当前字符的前一个字符。X=dh dl 删除当前字符， dl=x dh 删除前一个字符 dd 删除当前行 dj 删除上一行 dk 删除下一行 10d 删除当前行开始的10行。 D 删除当前字符至行尾。D=dd dd 删除当前字符之后的所有字符（本行） kdgg 删除当前行之前所有行（不包括当前行） jdG（jd shift + g） 删除当前行之后所有行（不包括当前行） :1,10d 删除1-10行 :11,d删除11行及以后所有的行:1,d 删除11行及以后所有的行 :1,d删除11行及以后所有的行:1,d 删除所有行 J(shift + j) 删除两行之间的空行，实际上是合并两行。 拷贝和粘贴 yy 拷贝当前行 nyy 拷贝当前后开始的n行，比如2yy拷贝当前行及其下一行。 p 在当前光标后粘贴,如果之前使用了yy命令来复制一行，那么就在当前行的下一行粘贴。 shift+p 在当前行前粘贴 :1,10 co 20 将1-10行插入到第20行之后。 :1,$ co $ 将整个文件复制一份并添加到文件尾部。 正常模式下按v（逐字）或V（逐行）进入可视模式，然后用jklh命令移动即可选择某些行或字符，再按y即可复制 ddp交换当前行和其下一行 xp交换当前字符和其后一个字符 剪切命令 正常模式下按v（逐字）或V（逐行）进入可视模式，然后用jklh命令移动即可选择某些行或字符，再按d即可剪切 ndd 剪切当前行之后的n行。利用p命令可以对剪切的内容进行粘贴 :1,10d 将1-10行剪切。利用p命令可将剪切后的内容进行粘贴。 :1, 10 m 20 将第1-10行移动到第20行之后。 退出命令 :wq 保存并退出 ZZ 保存并退出 :q! 强制退出并忽略所有更改 :e! 放弃所有修改，并打开原来文件。 窗口命令 :split或new 打开一个新窗口，光标停在顶层的窗口上 :split file或:new file 用新窗口打开文件 split打开的窗口都是横向的，使用vsplit可以纵向打开窗口。 Ctrl+ww 移动到下一个窗口 Ctrl+wj 移动到下方的窗口 Ctrl+wk 移动到上方的窗口 关闭窗口 :close 最后一个窗口不能使用此命令，可以防止意外退出vim。 :q 如果是最后一个被关闭的窗口，那么将退出vim。 ZZ 保存并退出。 关闭所有窗口，只保留当前窗口 :only 录制宏 按q键加任意字母开始录制，再按q键结束录制（这意味着vim中的宏不可嵌套），使用的时候@加宏名，比如qa。。。q录制名为a的宏，@a使用这个宏。 执行shell命令 :!command :!ls 列出当前目录下文件 :!perl -c script.pl 检查perl脚本语法，可以不用退出vim，非常方便。 :!perl script.pl 执行perl脚本，可以不用退出vim，非常方便。 :suspend或Ctrl – Z 挂起vim，回到shell，按fg可以返回vim。 注释命令 perl程序中#开始的行为注释，所以要注释某些行，只需在行首加入# 3,5 s/^/#/g 注释第3-5行 3,5 s/^#//g 解除3-5行的注释 1,$ s/^/#/g 注释整个文档。 :%s/^/#/g 注释整个文档，此法更快。 帮助命令 :help or F1 显示整个帮助 :help xxx 显示xxx的帮助，比如 :help i, :help CTRL-[（即Ctrl+[的帮助）。 :help ‘number’ Vim选项的帮助用单引号括起 :help 特殊键的帮助用&lt;&gt;扩起 :help -t Vim启动参数的帮助用- ：help i_ 插入模式下Esc的帮助，某个模式下的帮助用模式_主题的模式 帮助文件中位于||之间的内容是超链接，可以用Ctrl+]进入链接，Ctrl+o（Ctrl + t）返回 其他非编辑命令 . 重复前一次命令 :set ruler? 查看是否设置了ruler，在.vimrc中，使用set命令设制的选项都可以通过这个命令查看 :scriptnames 查看vim脚本文件的位置，比如.vimrc文件，语法文件及plugin等。 :set list 显示非打印字符，如tab，空格，行尾等。如果tab无法显示，请确定用set lcs=tab:&gt;-命令设置了.vimrc文件，并确保你的文件中的确有tab，如果开启了expendtab，那么tab将被扩展为空格。 Vim教程 在Unix系统上 $ vimtutor 在Windows系统上 :help tutor :syntax 列出已经定义的语法项 :syntax clear 清除已定义的语法规则 :syntax case match 大小写敏感，int和Int将视为不同的语法元素 :syntax case ignore 大小写无关，int和Int将视为相同的语法元素，并使用同样的配色方案","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jinyongzhu.github.io/categories/Linux/"},{"name":"vim","slug":"Linux/vim","permalink":"https://jinyongzhu.github.io/categories/Linux/vim/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://jinyongzhu.github.io/tags/Linux/"},{"name":"vim","slug":"vim","permalink":"https://jinyongzhu.github.io/tags/vim/"}]},{"title":"CentOS 7安装git服务端","slug":"CentOS-7安装git服务端","date":"2019-11-22T03:00:33.000Z","updated":"2019-11-22T03:01:49.246Z","comments":true,"path":"2019/1122/CentOS-7安装git服务端/","link":"","permalink":"https://jinyongzhu.github.io/2019/1122/CentOS-7%E5%AE%89%E8%A3%85git%E6%9C%8D%E5%8A%A1%E7%AB%AF/","excerpt":"","text":"系统版本：CentOS Linux release 7.6.1810 (Core) Git版本：git version 1.8.3.1 Git安装： 一、安装git和创建用户 安装Git $ yum install git 创建一个git用户组和用户，用来运行git服务 $ groupadd git $ adduser git -g git 禁止git用户登录: 修改/etc/passwd文件，修改 找到这句： git❌503:503::/home/git:/bin/bash 改为： git❌503:503::/home/git:/bin/git-shell 创建证书登录 mkdir /home/git/.ssh chmod 700 /home/git/.ssh touch 700 /home/git/.ssh/authorized_keys chmod 600 /home/git/.ssh/authorized_keys owner改为git chown -R git:git /home/git/.ssh/ 创建私钥，文件位于用户主目录里找到.ssh目录，里面有id_rsa和id_rsa.pub两个文件(服务器设置公私钥请参照网上有关资料) $ ssh-keygen -t rsa 创建完公私钥后将私钥文件放到客户端的~/.ssh目录下 二、初始化git仓库 cd /srv mkdir gitrepo chown git:git gitrepo/ cd gitrepo 创建一个空的Git仓库，服务器上的Git仓库通常都以.git结尾 $ git init project.git 将仓库所属用户改为git $ chown -R git:git project.git 三、安装客户端 下载地址：https://git-scm.com/downloads 按照安装界面一步步安装就可以了 在桌面上克隆服务器端project.git项目 打开git命令行，进入桌面 git clone git@xxx.xxx.xxx.xxx:xxxx/srv/gitrepo/project.git #接下来我们切换到project目录下，并手动创建1.txt(内自己写内容)文件 touch 1.txt #接下来，我们从工作区添加到版本库的暂存区、仓库区，上传到服务器 git add 1.txt git commit -m ‘new 1.txt’ git push origin master #至此，我们已经完成了基本的创建新文件并保存的任务 #注意事项： 在使用Git Push代码到数据仓库时，提示如下错误: [remote rejected] master -&gt; master (branch is currently checked out) 错误原型 remote: error: refusing to update checked out branch: refs/heads/master remote: error: By default, updating the current branch in a non-bare repository remote: error: is denied, because it will make the index and work tree inconsistent remote: error: with what you pushed, and will require ‘git reset --hard’ to match remote: error: the work tree to HEAD. 错误原因以及解决： 这是由于git默认拒绝了push操作，需要进行设置，修改/srv/gitrepo/project.git/.git/config文件后面添加如下代码： [receive] denyCurrentBranch = ignore 无法查看push后的git中文件的原因与解决方法: #在初始化远程仓库时最好使用 git --bare init 而不要使用：git init #如果使用了git init初始化，则远程仓库的目录下，也包含work tree，当本地仓库向远程仓库push时, 如果远程仓库正在push的分支上（如果当时不在push的分支，就没有问题）, 那么push后的结果不会反应在work tree上, 也即在远程仓库的目录下对应的文件还是之前的内容。 解决方法： #必须得使用命令git reset --hard才能看到push后的内容 搭建服务器上的GIT并实现自动同步到站点目录(www): #就比如刚才我们往远程仓库推送了index.php文件，虽然提示推送成功，但是我们现在在服务器端还看不到效果， 心理总是不爽。又比如我写了个html页面，我想在站点中马上看到，那自动同步就派上用场了。 #自动同步功能用到的是 git 的钩子功能， 服务器端：进入裸仓库：/srv/gitrepo/project.git/.git cd /srv/gitrepo/project.git/.git cd hooks //这里我们创建post-receive文件 vim post-receive //在该文件里输入以下内容 #!/bin/bash git --work-tree=/data/www checkout -f //保存退出后，将该文件用户及用户组都设置成git chown git:git post-receive //由于该文件其实就是一个shell文件，我们还应该为其设置可执行权限 chmod +x post-receive 如果你在Git推送的工程中发现推送成功 但是在www目录下并没有自己的代码，这时候你可要注意了：这是由于文件夹的权限的原因造成的！ 假设你的www目录的所属的用户组为root，你可以将你的git用户加入这个组;并给git添加写入权限，或者其他解决方法，反正你要服务器上的git用户有权限进入www文件夹。 cd /data/www gpasswd -a git root chmod 775 /data/www git reset --hard","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jinyongzhu.github.io/categories/Linux/"},{"name":"Git","slug":"Linux/Git","permalink":"https://jinyongzhu.github.io/categories/Linux/Git/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://jinyongzhu.github.io/tags/Linux/"},{"name":"CentOS 7","slug":"CentOS-7","permalink":"https://jinyongzhu.github.io/tags/CentOS-7/"},{"name":"Git","slug":"Git","permalink":"https://jinyongzhu.github.io/tags/Git/"}]},{"title":"CentOS 7源码安装LNMP环境","slug":"CentOS-7源码安装LNMP环境","date":"2019-11-22T02:58:19.000Z","updated":"2019-11-22T02:59:54.047Z","comments":true,"path":"2019/1122/CentOS-7源码安装LNMP环境/","link":"","permalink":"https://jinyongzhu.github.io/2019/1122/CentOS-7%E6%BA%90%E7%A0%81%E5%AE%89%E8%A3%85LNMP%E7%8E%AF%E5%A2%83/","excerpt":"","text":"系统版本：CentOS Linux release 7.6.1810 (Core) Nginx版本：nginx-1.15.9 PHP版本：php-7.1.0 MySQL版本：mysql-8.0.15 Nginx安装： yum install gcc gcc-c++ make zlib-devel pcre-devel openssl-devel cd /usr/local/src/ wget http://nginx.org/download/nginx-1.15.9.tar.gz tar zxf nginx-1.15.9.tar.gz &amp;&amp; cd nginx-1.15.9 ./configure --user=nginx --group=nginx --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx –conf-path=/etc/nginx/nginx.conf --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock –error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log –with-http_gzip_static_module --with-http_stub_status_module --with-http_ssl_module –with-pcre --with-file-aio --with-http_realip_module make &amp;&amp; make install cd /etc/init.d/ touch nginx vim nginx #编辑文件内容如下： #!/bin/sh nginx - this script starts and stops the nginx daemin chkconfig: - 85 15 description: Nginx is an HTTP(S) server, HTTP(S) reverse \\ proxy and IMAP/POP3 proxy server processname: nginx config: /etc/nginx/nginx.conf pidfile: /var/run/nginx.pid user: nginx Source function library. . /etc/rc.d/init.d/functions Source networking configuration. . /etc/sysconfig/network Check that networking is up. [ “$NETWORKING” = “no” ] &amp;&amp; exit 0 nginx=&quot;/usr/sbin/nginx&quot; prog=$(basename $nginx) NGINX_CONF_FILE=&quot;/etc/nginx/nginx.conf&quot; lockfile=/var/run/nginx.lock start() { [ -x $nginx ] || exit 5 [ -f $NGINX_CONF_FILE ] || exit 6 echo -n $&quot;Starting $prog: &quot; daemon $nginx -c NGINXCONFFILEretval=NGINX_CONF_FILE retval=NGINXC​ONFF​ILEretval=? echo [ $retval -eq 0 ] &amp;&amp; touch $lockfile return $retval } stop() { echo -n $&quot;Stopping $prog: &quot; killproc prog−QUITretval=prog -QUIT retval=prog−QUITretval=? echo [ $retval -eq 0 ] &amp;&amp; rm -f $lockfile return $retval } restart() { configtest || return $? stop start } reload() { configtest || return $? echo -n $&quot;Reloading $prog: &quot; killproc nginx−HUPRETVAL=nginx -HUP RETVAL=nginx−HUPRETVAL=? echo } force_reload() { restart } configtest() { $nginx -t -c $NGINX_CONF_FILE } rh_status() { status $prog } rh_status_q() { rh_status &gt;/dev/null 2&gt;&amp;1 } case “$1” in start) rh_status_q &amp;&amp; exit 0 $1 ;; stop) rh_status_q || exit 0 $1 ;; restart|configtest) $1 ;; reload) rh_status_q || exit 7 $1 ;; force-reload) force_reload ;; status) rh_status ;; condrestart|try-restart) rh_status_q || exit 0 ;; *) echo $“Usage: $0 {start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest}” exit 2 esac vim /etc/nginx/nginx.conf #在配置文件的default_type下面添加这两行内容： types_hash_bucket_size 64; server_names_hash_bucket_size 128; chmod +x nginx useradd -r -M -s /sbin/nologin nginx chkconfig --add nginx chkconfig --level 345 nginx on chkconfig --list nginx vim /etc/nginx/nginx.conf service nginx start PHP安装： yum install epel-release -y yum install libxml2 libxml2-devel openssl openssl-devel bzip2 bzip2-devel libcurl libcurl-devel libjpeg libjpeg-devel libpng libpng-devel freetype freetype-devel gmp gmp-devel libmcrypt libmcrypt-devel readline readline-devel libxslt libxslt-devel zlib zlib-devel glibc glibc-devel glib2 glib2-devel ncurses curl gdbm-devel db4-devel libXpm-devel libX11-devel gd-devel gmp-devel expat-devel xmlrpc-c xmlrpc-c-devel libicu-devel libmcrypt libmcrypt-devel libmemcached-devel -y cd /usr/local/src/ wget http://php.net/distributions/php-7.1.0.tar.gz tar zxf php-7.1.0.tar.gz &amp;&amp; cd php-7.1.0 ./configure --prefix=/usr/local/php --with-config-file-path=/etc --enable-fpm --with-fpm-user=www –with-fpm-group=www --enable -inline-optimization --disable-debug --disable-rpath --enable-shared –enable-soap --with-libxml-dir --with-xmlrpc --with-openssl --with-mcrypt --with-mhash --with-pcre-regex –with-sqlite3 --with-zlib --enable-bcmath --with-iconv --with-bz2 --enable-calendar --with-curl –with-cdb --enable-dom --enable-exif --enable-fileinfo --enable-filter --with-pcre-dir --enable-ftp –with-gd --with-openssl-dir --with-jpeg-dir --with-png-dir --with-zlib-dir --with-freetype-dir –enable-gd-native-ttf --enable-gd-jis-conv --with-gettext --with-gmp --with-mhash --enable-json –enable-mbstring --enable-mbregex --enable-mbregex-backtrack --with-libmbfl --with-onig --enable-pdo –with-mysqli=mysqlnd --with-pdo-mysql=mysqlnd --with-zlib-dir --with-pdo-sqlite --with-readline –enable-session --enable-shmop --enable-simplexml --enable-sockets --enable-sysvmsg --enable-sysvsem –enable-sysvshm --enable-wddx --with-libxml-dir --with-xsl --enable-zip --enable-mysqlnd-compression-support –with-pear --enable-opcache make &amp;&amp; make install /usr/local/php/bin/php -v useradd -r -M -s /sbin/nologin www PATH=$PATH:/usr/local/php/bin export PATH source /etc/profile cp php.ini-production /etc/php.ini cp /usr/local/php/etc/php-fpm.conf.default /usr/local/php/etc/php-fpm.conf cp /usr/local/php/etc/php-fpm.d/www.conf.default /usr/local/php/etc/php-fpm.d/www.conf cp sapi/fpm/init.d.php-fpm /etc/init.d/php-fpm chmod +x /etc/init.d/php-fpm chkconfig --add php-fpm chkconfig --list php-fpm service php-fpm start MySQL安装： yum -y install wget cmake gcc gcc-c++ ncurses ncurses-devel libaio-devel openssl openssl-devel cd /usr/local/src wget https://cdn.mysql.com//Downloads/MySQL-8.0/mysql-boost-8.0.15.tar.gz groupadd mysql useradd -r -g mysql -M -s /sbin/nologin mysql mkdir -p /usr/local/mysql mkdir -p /data/mysql tar zxf mysql-boost-8.0.15.tar.gz -C /usr/local/ cd /usr/local/mysql-8.0.15/ cmake . -DCMAKE_INSTALL_PREFIX=/usr/local/mysql -DMYSQL_DATADIR=/data/mysql/ -DSYSCONFDIR=/etc -DMYSQL_TCP_PORT=3306 -DWITH_BOOST=/usr/local/mysql-8.0.15/boost/ -DFORCE_INSOURCE_BUILD=1 make &amp;&amp; make install #配置mysql.cnf文件 vim /etc/my.cnf [mysqld] server-id=1 port=3306 basedir=/usr/local/mysql datadir=/data/mysql #目录权限修改 chown -R mysql:mysql /usr/local/mysql chown -R mysql:mysql /data/mysql chmod -R 755 /usr/local/mysql chmod -R 755 /data/mysql #初始化 /usr/local/mysql/bin/mysqld --initialize --user=mysql --datadir=/data/mysql/ /usr/local/mysql/bin/mysql_ssl_rsa_setup #启动MySQL /usr/local/mysql/bin/mysqld_safe --user=mysql &amp; #修改账号密码 /usr/local/mysql/bin/mysql -uroot -p mysql&gt; alter user ‘root’@‘localhost’ identified by “123456”; mysql&gt; show databases; #添加远程特账号 mysql&gt; create user root@’%’ identified by ‘123456’; mysql&gt; grant all privileges on . to root@’%’; mysql&gt; flush privileges; #创建软链接(非必要) ln -s /usr/local/mysql/bin/* /usr/local/bin/ mysql -h 127.0.0.1 -P 3306 -uroot -p123456 -e “select version();” 9#添加到启动项(非必要) cp support-files/mysql.server /etc/init.d/mysqld chmo+x /etc/init.d/mysqld chkconfig --add mysqld chkconfig --list mysqld /etc/init.d/mysqld stop /etc/init.d/mysqld start","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jinyongzhu.github.io/categories/Linux/"},{"name":"CentOS 7","slug":"Linux/CentOS-7","permalink":"https://jinyongzhu.github.io/categories/Linux/CentOS-7/"},{"name":"LNMP","slug":"Linux/CentOS-7/LNMP","permalink":"https://jinyongzhu.github.io/categories/Linux/CentOS-7/LNMP/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://jinyongzhu.github.io/tags/Linux/"},{"name":"LNMP","slug":"LNMP","permalink":"https://jinyongzhu.github.io/tags/LNMP/"},{"name":"CentOS 7","slug":"CentOS-7","permalink":"https://jinyongzhu.github.io/tags/CentOS-7/"}]},{"title":"如何检查Linux服务器受到DDOS攻击","slug":"如何检查Linux服务器受到DDOS攻击","date":"2019-11-22T02:54:14.000Z","updated":"2019-11-22T02:56:30.719Z","comments":true,"path":"2019/1122/如何检查Linux服务器受到DDOS攻击/","link":"","permalink":"https://jinyongzhu.github.io/2019/1122/%E5%A6%82%E4%BD%95%E6%A3%80%E6%9F%A5Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%8F%97%E5%88%B0DDOS%E6%94%BB%E5%87%BB/","excerpt":"","text":"登录到你的服务器以root用户执行下面的命令，使用它你可以检查你的服务器是在DDOS攻击与否： netstat -anp |grep ‘tcp|udp’ | awk ‘{print $5}’ | cut -d: -f1 | sort | uniq -c | sort –n 该命令将显示已登录的是连接到服务器的最大数量的IP的列表。 DDOS变得更为复杂，因为攻击者在使用更少的连接，更多数量IP的攻击服务器的情况下，你得到的连接数量较少，即使你的服务器被攻击了。有一点很重要，你应该检查当前你的服务器活跃的连接信息，执行以下命令： netstat -n | grep :80 |wc –l 上面的命令将显示所有打开你的服务器的活跃连接。 您也可以使用如下命令： netstat -n | grep :80 | grep SYN |wc –l 从第一个命令有效连接的结果会有所不同，但如果它显示连接超过500，那么将肯定有问题。 如果第二个命令的结果是100或以上，那么服务器可能被同步攻击。 一旦你获得了攻击你的服务器的IP列表，你可以很容易地阻止它。 同构下面的命令来阻止IP或任何其他特定的IP： route add ipaddress reject 一旦你在服务器上阻止了一个特定IP的访问，你可以检查对它的阻止是否有效 通过使用下面的命令： route -n |grep IPaddress 您还可以通过使用下面的命令，用iptables封锁指定的IP。 iptables -A INPUT 1 -s IPADRESS -j DROP/REJECT service iptables restart service iptables save 上面的命令执行后，停止httpd连接，重启httpd服务。 使用下面的命令： killall -KILL httpd service httpd startssl","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jinyongzhu.github.io/categories/Linux/"},{"name":"DDoS","slug":"Linux/DDoS","permalink":"https://jinyongzhu.github.io/categories/Linux/DDoS/"}],"tags":[{"name":"DDoS","slug":"DDoS","permalink":"https://jinyongzhu.github.io/tags/DDoS/"}]},{"title":"mysqldump Got error 1556 You can not use locks with log tables","slug":"mysqldump-Got-error-1556-You-can-not-use-locks-with-log-tables","date":"2019-11-22T02:44:01.000Z","updated":"2019-11-22T02:45:28.034Z","comments":true,"path":"2019/1122/mysqldump-Got-error-1556-You-can-not-use-locks-with-log-tables/","link":"","permalink":"https://jinyongzhu.github.io/2019/1122/mysqldump-Got-error-1556-You-can-not-use-locks-with-log-tables/","excerpt":"","text":"问题：mysqldump: Got error: 1556: You can’t use locks with log tables. 在老男孩带学生做主从同步实践时，发现学生实践操作时遇到如下问题无法解决，于是，老男孩把解决的过程总结如下： [root@Oldboy ~]# mysqldump -uroot -p’oldboy’ -S /data/3306/mysql.sock -A -B &gt;a.sql mysqldump: Got error: 1556: You can’t use locks with log tables. when using LOCK TABLES 解决过程： 同样的操作，其他10几个学生都是OK的，只有这个学生有问题，在询问后，并查看了相关配置过程后，联系到往期也有学生发生过类似问题，于是，发现了原因。 [root@Oldboy ~]# which mysqldump /usr/bin/mysqldump 发现原因,mysql的安装路径为/application/mysql，查看mysqldump的路径应该是/application/mysql/bin才对，但是现在的路径是rpm包安装的mysql命令路径了，至此原因找到。 [root@oldboy ~]# tail -1 /etc/profile export PATH=PATH:/application/mysql/bin 在/etc/profile文件中，mysql的命令所在路径，该学生放到了结尾，因此，当执行mysqldump命令时，优先找到了rpm包自带的/usr/bin/mysqldump命令，从而导致了错误。 我们把把mysql的命令路径放到PATH变量的最前面： [root@oldboy ~]# tail -1 /etc/profile export PATH=/application/mysql/bin:PATH [root@Oldboy ~]# . /etc/profile [root@Oldboy ~]# echo $PATH /application/mysql/bin:/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/usr/local/mysql/bin:/root/bin:/usr/local/mysql/bin:/application/mysql5.1.65/bin:/application/apache/bin [root@Oldboy ~]# which mysqldump /application/mysql/bin/mysqldump 此时在导出数据库： [root@Oldboy ~]#mysqldump -uroot -p’oldboy’ -S /data/3306/mysql.sock -A -B &gt;a.sql 可以正常导出了。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://jinyongzhu.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://jinyongzhu.github.io/tags/MySQL/"}]},{"title":"深入理解mysql锁机制--全局锁，表级锁，行锁","slug":"深入理解mysql锁机制-全局锁，表级锁，行锁","date":"2019-11-22T02:35:30.000Z","updated":"2019-11-22T02:41:55.443Z","comments":true,"path":"2019/1122/深入理解mysql锁机制-全局锁，表级锁，行锁/","link":"","permalink":"https://jinyongzhu.github.io/2019/1122/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3mysql%E9%94%81%E6%9C%BA%E5%88%B6-%E5%85%A8%E5%B1%80%E9%94%81%EF%BC%8C%E8%A1%A8%E7%BA%A7%E9%94%81%EF%BC%8C%E8%A1%8C%E9%94%81/","excerpt":"","text":"概述 根据加锁的范围，MySQL 里面的锁大致可以分成全局锁，表级锁，行锁。 mysql 锁管理机制 1、全局锁 全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是Flush tables with read lock (FTWRL)。 当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。 1.1 全局锁使用场景 全局锁的典型使用场景是，做全库逻辑备份（mysqldump）。重新做主从时候 也就是把整库每个表都 select 出来存成文本。 以前有一种做法，是通过 FTWRL 确保不会有其他线程对数据库做更新，然后对整个库做备份。注意，在备份过程中整个库完全处于只读状态。 数据库只读状态的危险性： 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就能停止。如果你在从库上备份，那么备份期间从库不能执行主库同步过来的binlog，会导致主从延迟。 注：上面逻辑备份，是不加–single-transaction参数 看来加全局锁不太好。但是细想一下，备份为什么要加锁呢？来看一下不加锁会有什么问题？ 1.2 不加锁产生的问题 比如手机卡，购买套餐信息 这里分为两张表 u_acount (用于余额表)，u_pricing (资费套餐表) 步骤: 1 . u_account 表中数据 用户A 余额：300 u_pricing 表中数据 用户A 套餐：空 2. 发起备份，备份过程中先备份u_account表，备份完了这个表，这个时候u_account 用户余额是300 3. 这个时候套用户购买了一个资费套餐100，餐购买完成，写入到u_print套餐表购买成功，备份期间的数据。 4. 备份完成 可以看到备份的结果是，u_account 表中的数据没有变， u_pricing 表中的数据已经购买了资费套餐100。也就是说，不加锁的话，备份系统备份的得到的库不是一个逻辑时间点，这个数据是逻辑不一致的。 1.3 为什么需要全局读锁(FTWRL) 可能有的人在疑惑，官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数–single-transaction的时候，导数据之前就会启动一个事务，来确保拿到一致性快照视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。 为什么还需要 FTWRL 呢？ 一致性读是好，但前提是引擎要支持这个隔离级别。比如，对于 MyISAM 这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用FTWRL 命令了。 所以，single-transaction 方法只适用于所有的表使用事务引擎的库。如果有的表使用了不支持事务的引擎，那么备份就只能通过 FTWRL 方法。这往往是 DBA 要求业务开发人员使用 InnoDB 替代 MyISAM 的原因之一。 1.4 全局锁两种方法 一.FLUSH TABLES WRITE READ LOCK 二.set global readonly=true 既然要全库只读，为什么不使用 set global readonly=true 的方式呢？确实 readonly 方式也可以让全库进入只读状态，但我还是会建议你用 FTWRL 方式，主要有几个原因： 一是，在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，我不建议你使用。 二是，在异常处理机制上有差异。如果执行FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。 三是，readonly 对super用户权限无效 注 ：业务的更新不只是增删改数据（DML)，还有可能是加字段等修改表结构的操作（DDL）。不论是哪种方法，一个库被全局锁上以后，你要对里面任何一个表做加字段操作，都是会被锁住的。 即使没有被全局锁住，加字段也不是就能一帆风顺的，还有表级锁了 2、表级锁 MySQL 中表级别锁有两种：一种是普通表锁，一种是元数据锁(metadata lock. MDL) 表锁的语法是 lock tables xxx read/write 同样使用 unlock tables 来释放锁。通过加读锁我们可以限制其他语句进行写入，但是重复加读锁不受影响。但是当我们加写锁的时候，既不可以读也不可以写。同样在使用 unlock tables 之后可以解除锁定。 另外一种表级锁是 MDL 锁(metadata lock) MDL 锁不需要显示的使用，在访问一个表的时候自动就被加上了。 MDL 锁是用来保证读写正确性的，当我们对一个表在做 增删改查操作的时候都会被加上 MDL 读锁。当要进行 ddl 的时候需要加 MDL 写锁。 MDL 读锁与读锁之间不互斥，因此我们可以多个线程进程对一个表进行增删改查。 MDL 读写锁之间互斥，用来保证表结构变更的安全性。因此如果有两个线程同时要给同一个表加字段，其中一个要等另外一个执行完成之后再开始执行。 下面我们来看一个比较有代表性的场景 MDL 读锁写锁互斥导致表无法读写被死锁。 session A: 开始一个事务，然后查询 t 表，这会给 t 表加上 MDL 读锁。（注意该事务被打开后就一直没有结束） session B: 查询一个 t 表。这里应该是 autoocommit 会自动成功。 session C: 修改表 ddl 会加 MDL 写锁，和 session A 的读锁互斥。这个时候就锁住了表。 session D: 由于 session C 造成了写锁阻塞，所以后面所有的请求都会被锁住。 如果该表查询频繁，而且客户端有重试的机制，那么这个数据库的查询线程会很快被打满。 可能在进行 web 开发的同学会经常遇到类似的情况。比如我在 ipython 里面打开了一个数据库某个表的连接，然后我一直没有 commit 。就可能造成该表在加写锁的时候阻塞后面所有的操作。 这种事情非常常见。 那么我们如何安全的给小表加字段，首先我们应该解决长事务或者脚本事务的问题，因为他们会一直挂读锁不结束。在 MySQL 的 information_schema 中的 innodb_trx 中可以查询到执行中的长事务，但是比较麻烦的是这个看不到很短的事务。但是往往进行 sleep 的短事务也可能因为一直没有 commit 而导致上面的情况出现。 这个时候就需要把对应表的 sleep 进程 kill 掉使其恢复正常。 3、行级锁 先来看个描述两阶段锁的例子： 事务 A 会持有两条记录的行锁，并且只会在 commit 之后才会释放。 在 InnoDB 事务中，行锁是在需要的时候加上，但是并不是不需要就立刻释放，而是等事务结束之后才会释放。这个就是两阶段锁协议。 知道了这个设定我们应该在长事务中把影响并发度的锁尽量往后放。下面的这一段的介绍比较复杂，我觉得丁奇讲得还是比较清楚的所以直接引用原文了。 假设你负责实现一个电影票在线交易业务，顾客 A 要在影院 B 购买电影票。我们简化一点，这个业务需要涉及到以下操作： 从顾客 A 账户余额中扣除电影票价； 给影院 B 的账户余额增加这张电影票价； 记录一条交易日志。 也就是说，要完成这个交易，我们需要 update 两条记录，并 insert 一条记录。当然，为了保证交易的原子性，我们要把这三个操作放在一个事务中。那么，你会怎样安排这三个语句在事务中的顺序呢？ 试想如果同时有另外一个顾客 C 要在影院 B 买票，那么这两个事务冲突的部分就是语句 2 了。因为它们要更新同一个影院账户的余额，需要修改同一行数据。 根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。所以，如果你把语句 2 安排在最后，比如按照 3、1、2 这样的顺序，那么影院账户余额这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://jinyongzhu.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://jinyongzhu.github.io/tags/MySQL/"}]},{"title":"Nginx 502错误触发条件与解决办法汇总","slug":"Nginx-502错误触发条件与解决办法汇总","date":"2019-11-22T02:16:35.000Z","updated":"2019-11-22T02:18:16.705Z","comments":true,"path":"2019/1122/Nginx-502错误触发条件与解决办法汇总/","link":"","permalink":"https://jinyongzhu.github.io/2019/1122/Nginx-502%E9%94%99%E8%AF%AF%E8%A7%A6%E5%8F%91%E6%9D%A1%E4%BB%B6%E4%B8%8E%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%E6%B1%87%E6%80%BB/","excerpt":"","text":"一些运行在Nginx上的网站有时候会出现“502 Bad Gateway”错误，有些时候甚至频繁的出现。有些站长是在刚刚转移到Nginx之后就出现了这个问题，所以经常会怀疑这是不是Nginx的问题，但事实上这是个误区。 以下是从张宴和Ayou的博客搜集整理的一些Nginx 502错误的排查方法，供大家参考： Nginx 502错误的原因比较多，是因为在代理模式下后端服务器出现问题引起的。这些错误一般都不是nginx本身的问题，一定要从后端找原因！但nginx把这些出错都揽在自己身上了，着实让nginx的推广者备受置疑，毕竟从字眼上理解，bad gateway？不就是bad nginx吗？让不了解的人看到，会直接把责任推在nginx身上，希望nginx下一个版本会把出错提示写稍微友好一些，至少不会是现在简单的一句502 Bad Gateway，另外还不忘附上自己的大名。 Nginx 502的触发条件 502错误最通常的出现情况就是后端主机当机。在upstream配置里有这么一项配置：proxy_next_upstream，这个配置指定了nginx在从一个后端主机取数据遇到何种错误时会转到下一个后端主机，里头写上的就是会出现502的所有情况拉，默认是error timeout。error就是当机、断线之类的，timeout就是读取堵塞超时，比较容易理解。我一般是全写上的： proxy_next_upstream error timeout invalid_header http_500 http_503; 不过现在可能我要去掉http_500这一项了，http_500指定后端返回500错误时会转一个主机，后端的jsp出错的话，本来会打印一堆stacktrace的错误信息，现在被502取代了。但公司的程序员可不这么认为，他们认定是nginx出现了错误，我实在没空跟他们解释502的原理了…… 503错误就可以保留，因为后端通常是apache resin，如果apache死机就是error，但resin死机，仅仅是503，所以还是有必要保留的。 解决办法 遇到502问题，可以优先考虑按照以下两个步骤去解决。 1、查看当前的PHP FastCGI进程数是否够用： netstat -anpo | grep “php-cgi” | wc -l 如果实际使用的“FastCGI进程数”接近预设的“FastCGI进程数”，那么，说明“FastCGI进程数”不够用，需要增大。 2、部分PHP程序的执行时间超过了Nginx的等待时间，可以适当增加nginx.conf配置文件中FastCGI的timeout时间，例如： … http { … fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; … } … php.ini中memory_limit设低了会出错，修改了php.ini的memory_limit为64M，重启nginx，发现好了，原来是PHP的内存不足了。 如果这样修改了还解决不了问题，可以参考下面这些方案： 一、max-children和max-requests 一台服务器上运行着nginx php(fpm) xcache，访问量日均 300W pv左右 最近经常会出现这样的情况： php页面打开很慢，cpu使用率突然降至很低，系统负载突然升至很高，查看网卡的流量，也会发现突然降到了很低。这种情况只持续数秒钟就恢复了 检查php-fpm的日志文件发现了一些线索 Sep 30 08:32:23.289973 [NOTICE] fpm_unix_init_main(), line 271: getrlimit(nofile): max:51200, cur:51200 Sep 30 08:32:23.290212 [NOTICE] fpm_sockets_init_main(), line 371: using inherited socket fd=10, “127.0.0.1:9000″ Sep 30 08:32:23.290342 [NOTICE] fpm_event_init_main(), line 109: libevent: using epoll Sep 30 08:32:23.296426 [NOTICE] fpm_init(), line 47: fpm is running, pid 30587 在这几句的前面，是1000多行的关闭children和开启children的日志 原来，php-fpm有一个参数 max_requests，该参数指明了，每个children最多处理多少个请求后便会被关闭，默认的设置是500。因为php是把请求轮询给每个children，在大流量下，每个childre到达max_requests所用的时间都差不多，这样就造成所有的children基本上在同一时间被关闭。 在这期间，nginx无法将php文件转交给php-fpm处理，所以cpu会降至很低(不用处理php，更不用执行sql)，而负载会升至很高(关闭和开启children、nginx等待php-fpm)，网卡流量也降至很低(nginx无法生成数据传输给客户端) 解决问题很简单，增加children的数量，并且将 max_requests 设置未 0 或者一个比较大的值： 打开 /usr/local/php/etc/php-fpm.conf 调大以下两个参数(根据服务器实际情况，过大也不行） 5120 600 然后重启php-fpm。 二、增加缓冲区容量大小 将nginx的error log打开，发现“pstream sent too big header while reading response header from upstream”这样的错误提示。查阅了一下资料，大意是nginx缓冲区有一个bug造成的,我们网站的页面消耗占用缓冲区可能过大。参考老外写的修改办法增加了缓冲区容量大小设置，502问题彻底解决。后来系统管理员又对参数做了调整只保留了2个设置参数：client head buffer，fastcgi buffer size。 三、request_terminate_timeout 如果主要是在一些post或者数据库操作的时候出现502这种情况，而不是在静态页面操作中常见，那么可以查看一下php-fpm.conf设置中的一项： request_terminate_timeout 这个值是max_execution_time，就是fast-cgi的执行脚本时间。 0s 0s为关闭，就是无限执行下去。（当时装的时候没仔细看就改了一个数字） 发现，问题解决了，执行很长时间也不会出错了。 优化fastcgi中，还可以改改这个值5s 看看效果。 php-cgi进程数不够用、php执行时间长、或者是php-cgi进程死掉，都会出现502错误。 如果您还有其他的解决方法，欢迎与编辑沟通！当然，如果你的网站并发量的确很大，那么最终也许需要寻求系统级的解决办法……","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jinyongzhu.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://jinyongzhu.github.io/tags/Linux/"},{"name":"Nginx","slug":"Nginx","permalink":"https://jinyongzhu.github.io/tags/Nginx/"}]},{"title":"Python备份MySQL数据库脚本","slug":"Python备份MySQL数据库脚本","date":"2019-11-20T08:18:38.000Z","updated":"2019-11-21T06:56:28.446Z","comments":true,"path":"2019/1120/Python备份MySQL数据库脚本/","link":"","permalink":"https://jinyongzhu.github.io/2019/1120/Python%E5%A4%87%E4%BB%BDMySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E8%84%9A%E6%9C%AC/","excerpt":"","text":"#!/usr/bin/env python3 import os import time import datetime 定义服务器，用户名、密码、数据库名称（多个库分行放置）和备份的路径 DB_USER = ‘root’ DB_PASSWD = ‘root7895123’ DB_NAME = ‘/root/dbnames.txt’ MYSQLDUMP = ‘/usr/local/mysql/bin/mysqldump’ BACKUP_PATH = ‘/data/backup/mysql/’ DATETIME = time.strftime(’%Y%m%d-%H%M%S’) TODAYBACKUPPATH = BACKUP_PATH + DATETIME print(“createing backup folder!”) 创建备份文件夹 if not os.path.exists(TODAYBACKUPPATH): os.makedirs(TODAYBACKUPPATH) print(“checking for databases names file”) 定义执行备份脚本，读取文件中的数据库名称，注意按行读写，不校验是否存在该库 def run_backup(): in_file = open(DB_NAME,“r”) for dbname in in_file.readlines(): dbname = dbname.strip() print(“now starting backup database %s” %dbname) dumpcmd = MYSQLDUMP+&quot; -u&quot; +DB_USER + &quot; -p&quot;+DB_PASSWD+&quot; &quot; +dbname+&quot; &gt; “+TODAYBACKUPPATH +”/&quot;+dbname+&quot;.sql&quot; os.system(dumpcmd) in_file.close() 执行压缩的函数 def run_tar(): compress_file = TODAYBACKUPPATH + “.tar.gz” compress_cmd = “tar -czvf &quot; +compress_file+” &quot;+DATETIME os.chdir(BACKUP_PATH) os.system(“pwd”) os.system(compress_cmd) print(“compress complete!”) # 删除备份文件夹 remove_cmd = &quot;rm -rf &quot;+TODAYBACKUPPATH os.system(remove_cmd) 备份数据库文件存在就执行备份和压缩,否则退出 if os.path.exists(DB_NAME): file1 = open(DB_NAME) print(&quot;starting backup of all db listed in file &quot;+DB_NAME) run_backup() run_tar() print(“backup success!”) else: print(“database file not found…”) exit() Python脚本备份定时任务 30 9 * * * /usr/local/bin/python3 /usr/local/sbin/python_backup.py &gt; /dev/null 2&gt;&amp;1","categories":[{"name":"Python","slug":"Python","permalink":"https://jinyongzhu.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://jinyongzhu.github.io/tags/Python/"},{"name":"MySQL","slug":"MySQL","permalink":"https://jinyongzhu.github.io/tags/MySQL/"}]},{"title":"nginx配置Google海外代理","slug":"nginx配置Google海外代理","date":"2019-11-20T06:58:02.000Z","updated":"2019-11-21T06:56:11.318Z","comments":true,"path":"2019/1120/nginx配置Google海外代理/","link":"","permalink":"https://jinyongzhu.github.io/2019/1120/nginx%E9%85%8D%E7%BD%AEGoogle%E6%B5%B7%E5%A4%96%E4%BB%A3%E7%90%86/","excerpt":"","text":"proxy_cache_path /run/shm levels=1:2 keys_zone=one:100m max_size=1g; proxy_cache_key hosthosthostrequest_uri; upstream google { server 216.58.221.142 max_fails=3 fail_timeout=10s; //IP自己添加 } server { listen 80; server_name g.ttlsa.com; location / { proxy_pass http://google; proxy_cache one; proxy_cache_valid 200 302 2h; proxy_cache_valid 404 1h; proxy_buffering off; proxy_cookie_domain google.com g.ttlsa.com; proxy_redirect https://www.google.com/ /; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Accept-Encoding &quot;&quot;; proxy_set_header User-Agent $http_user_agent; sub_filter www.google.com g.ttlsa.com; sub_filter_once off; } }","categories":[{"name":"Linux","slug":"Linux","permalink":"https://jinyongzhu.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://jinyongzhu.github.io/tags/Linux/"},{"name":"Nginx","slug":"Nginx","permalink":"https://jinyongzhu.github.io/tags/Nginx/"}]}]}